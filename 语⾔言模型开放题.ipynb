{
 "cells": [
  {
   "cell_type": "markdown",
   "metadata": {
    "id": "420B3C65427046218B0EEE4F1A06425C",
    "jupyter": {},
    "mdEditEnable": false,
    "slideshow": {
     "slide_type": "slide"
    },
    "tags": []
   },
   "source": [
    "# 语言模型开放题"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "id": "6AA2434B76F8469283BB4E3924CC54DA",
    "jupyter": {},
    "mdEditEnable": false,
    "slideshow": {
     "slide_type": "slide"
    },
    "tags": []
   },
   "source": [
    "# 数据集\n",
    "本次开放题将与课程内容保持一致，将周杰伦从第一张专辑《Jay》到第十张专辑《跨时代》中的歌词作为训练语言模型的数据集，来训练一个语言模型，并在模型训练好后，用这个模型来创作新的歌词。\n",
    "\n",
    "这里简介将此数据集转换成字符级循环神经网络及其变体所需要的输入格式的方法：\n",
    "\n",
    "### 读取数据集\n",
    "\n",
    "首先读取这个数据集，为了打印方便，我们把换行符替换成空格，并且打印前40个字符。"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 3,
   "metadata": {
    "attributes": {
     "classes": [],
     "id": "",
     "n": "20"
    },
    "id": "93634041065144A28019BB51100945BB",
    "jupyter": {},
    "scrolled": false,
    "slideshow": {
     "slide_type": "slide"
    },
    "tags": []
   },
   "outputs": [
    {
     "data": {
      "text/plain": [
       "'想要有直升机 想要和你飞到宇宙去 想要和你融化在一起 融化在宇宙里 我每天每天每'"
      ]
     },
     "execution_count": 3,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "with open('Datasets/jaychou_lyrics.txt', encoding=\"utf-8\") as f:\n",
    "        corpus_chars = f.read()\n",
    "corpus_chars = corpus_chars.replace('\\n', ' ').replace('\\r', ' ')\n",
    "\n",
    "corpus_chars[:40]"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "id": "7D030E957391442CAA0F8EB9C636F659",
    "jupyter": {},
    "mdEditEnable": false,
    "slideshow": {
     "slide_type": "slide"
    },
    "tags": []
   },
   "source": [
    "### 建立字符索引\n",
    "\n",
    "我们将每个字符映射成一个从0开始的连续整数作为索引，以方便之后的数据处理。\n",
    "\n",
    "为了得到索引，我们将数据集里所有不同字符取出来，然后将其逐一映射到索引来构造词典。\n",
    "\n",
    "接着，打印`vocab_size`，即词典中不同字符的个数，又称词典大小。\n",
    "\n",
    "之后，将训练数据集中每个字符转化为索引，并打印前20个字符及其对应的索引。\n",
    "\n",
    "建立字符索引之后我们就可以对于数据集进行操作了。"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 4,
   "metadata": {
    "attributes": {
     "classes": [],
     "id": "",
     "n": "9"
    },
    "id": "1D083F84446D4BD69139F40FAB6ECE24",
    "jupyter": {},
    "scrolled": false,
    "slideshow": {
     "slide_type": "slide"
    },
    "tags": []
   },
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "vocab_size: 2582\n",
      "chars: 想要有直升机 想要和你飞到宇宙去 想要和\n",
      "indices: [1049, 1569, 553, 1579, 521, 1185, 2550, 1049, 1569, 2035, 992, 959, 208, 2423, 784, 373, 2550, 1049, 1569, 2035]\n"
     ]
    }
   ],
   "source": [
    "idx_to_char = list(set(corpus_chars))\n",
    "char_to_idx = dict([(char, i) for i, char in enumerate(idx_to_char)])\n",
    "vocab_size = len(char_to_idx)\n",
    "print('vocab_size:', vocab_size)\n",
    "corpus_indices = [char_to_idx[char] for char in corpus_chars]\n",
    "sample = corpus_indices[:20]\n",
    "print('chars:', ''.join([idx_to_char[idx] for idx in sample]))\n",
    "print('indices:', sample)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "id": "2F3E9687D7D040CB8CCF4D10405C7969",
    "jupyter": {},
    "mdEditEnable": false,
    "slideshow": {
     "slide_type": "slide"
    },
    "tags": []
   },
   "source": [
    "# 评价指标\n",
    "\n",
    "在本次大作业中，我们使用困惑度（perplexity）来评价语言模型的好坏，困惑度是对交叉熵损失函数做指数运算后得到的值。\n",
    "\n",
    "* 最佳情况下，模型总是把标签类别的概率预测为1，此时困惑度为1；\n",
    "* 最坏情况下，模型总是把标签类别的概率预测为0，此时困惑度为正无穷；\n",
    "* 基线情况下，模型总是预测所有类别的概率都相同，此时困惑度为类别个数。\n",
    "\n",
    "显然，任何一个有效模型的困惑度必须小于类别个数。在语言模型中，困惑度必须小于词典大小`vocab_size`。"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 5,
   "metadata": {},
   "outputs": [],
   "source": [
    "import math\n",
    "def perplexity(l_sum, n):\n",
    "    return math.exp(l_sum / n)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "id": "5CC76AEA522E4882BC23EB584B616BE4",
    "jupyter": {},
    "mdEditEnable": false,
    "slideshow": {
     "slide_type": "slide"
    },
    "tags": []
   },
   "source": [
    "# 语言模型选取\n",
    "\n",
    "以下进入本次大作业的正题——语言模型的分析与比较。\n",
    "\n",
    "学员需要根据每个知识点介绍之后的问题进行理论分析、模型实现、结果汇总以及指标分析，尽可能完善地阐述实验收获以回答问题，并且以撰写报告的形式汇总自己的研究成果。"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 6,
   "metadata": {},
   "outputs": [],
   "source": [
    "import torch\n",
    "import numpy as np"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## 循环神经网络\n",
    "\n",
    "在$n$元语法中，时间步$t$的词$w_t$基于前面所有词的条件概率只考虑了最近时间步的$n-1$个词。如果要考虑比$t-(n-1)$更早时间步的词对$w_t$的可能影响，模型参数的数量将随之呈指数级增长。\n",
    "\n",
    "循环神经网络（recurrent neural network，RNN）并非刚性地记忆所有固定长度的序列，而是通过隐藏状态来存储之前时间步的信息。具体来说，在循环神经网络中，时间步$t$的隐藏变量的计算由当前时间步的输入和上一时间步的隐藏变量共同决定：\n",
    "\n",
    "$${H}_t = \\phi({X}_t {W}_{xh} + {H}_{t-1} {W}_{hh}  + {b}_h).$$\n",
    "\n",
    "如下图所示，隐藏变量能够捕捉截至当前时间步的序列的历史信息，就像是神经网络当前时间步的状态或记忆一样。\n",
    "\n",
    "![Image Name](https://zh.gluon.ai/_images/rnn.svg)\n",
    "\n",
    "我们使用Pytorch中的`nn.RNN`来构造循环神经网络。在本节中，我们主要关注`nn.RNN`的以下几个构造函数参数：\n",
    "\n",
    "* `input_size` - The number of expected features in the input x\n",
    "* `hidden_size` – The number of features in the hidden state h\n",
    "* `nonlinearity` – The non-linearity to use. Can be either 'tanh' or 'relu'. Default: 'tanh'\n",
    "* `batch_first` – If True, then the input and output tensors are provided as (batch_size, num_steps, input_size). Default: False\n",
    "\n",
    "这里的`batch_first`决定了输入的形状，我们使用默认的参数`False`，对应的输入形状是 (num_steps, batch_size, input_size)。\n",
    "\n",
    "`forward`函数的参数为：\n",
    "\n",
    "* `input` of shape (num_steps, batch_size, input_size): tensor containing the features of the input sequence. \n",
    "* `h_0` of shape (num_layers * num_directions, batch_size, hidden_size): tensor containing the initial hidden state for each element in the batch. Defaults to zero if not provided. If the RNN is bidirectional, num_directions should be 2, else it should be 1.\n",
    "* `forward`函数的返回值是：\n",
    "    * `output` of shape (num_steps, batch_size, num_directions * hidden_size): tensor containing the output features (h_t) from the last layer of the RNN, for each t.\n",
    "    * `h_n` of shape (num_layers * num_directions, batch_size, hidden_size): tensor containing the hidden state for t = num_steps.\n",
    "\n",
    "现在我们构造一个`nn.RNN`实例，并用一个简单的例子来看一下输出的形状。"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "id": "4F5454D7A46144D186F770924480B436",
    "jupyter": {},
    "mdEditEnable": false,
    "slideshow": {
     "slide_type": "slide"
    },
    "tags": []
   },
   "source": [
    "### 问题一：\n",
    "- 解释为什么循环神经网络可以表达某时间步的词基于文本序列中所有过去的词的条件概率？\n",
    "- 调节循环神经网络的超参数以及深度，分析不同超参和深度下，训练时间、语言模型困惑度（perplexity）以及创作歌词的结果等相关指标的变化，并以表格的形式进行总结。"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "#### RNN可表达过去词的条件概率\n",
    "循环神经网络引入了隐藏变量$H_t$，`RNN`神经网络的输入包括代表历史信息的$H_{t-1}$和代表本时刻的$X_t$，$H_t$又可以作为下一时刻的输入，因此$H_t$能够捕捉截至当前时间步序列的历史信息。"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### RNN模型的超参数\n",
    "- rnn_layer\n",
    "- hidden_size\n",
    "- bidirectional"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 7,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "2582"
      ]
     },
     "execution_count": 7,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "vocab_size"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "class RNNModel(torch.nn.Module):\n",
    "    def __init__(self, rnn_layer, vocab_size):\n",
    "        super(RNNModel, self).__init__()\n",
    "        self.rnn = torch.nn.RNN\n",
    "        "
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "id": "CC32E514DB82434B80891531DD242AA7",
    "jupyter": {},
    "mdEditEnable": false,
    "slideshow": {
     "slide_type": "slide"
    },
    "tags": []
   },
   "source": [
    "## 门控循环神经网络\n",
    "\n",
    "\n",
    "门控循环神经网络（gated recurrent neural network）通过可以学习的门来控制信息的流动，可以更好地捕捉时间序列中时间步距离较大的依赖关系。\n",
    "\n",
    "\n",
    "### 门控循环单元（GRU）\n",
    "\n",
    "\n",
    "\n",
    "\n",
    "\n",
    "门控循环单元（gated recurrent unit，GRU）是一种常用的门控循环神经网络[1, 2]，它引入了重置门（reset gate）和更新门（update gate）的概念，并且计算候选隐藏状态，从而修改了循环神经网络中隐藏状态的计算方式。\n",
    "\n",
    "* 重置门控制了上一时间步的隐藏状态如何流入当前时间步的候选隐藏状态，有助于捕捉时间序列里短期的依赖关系；\n",
    "* 更新门可以控制隐藏状态应该如何被包含当前时间步信息的候选隐藏状态所更新，有助于捕捉时间序列里长期的依赖关系。\n",
    "\n",
    "如下图所示，假设更新门在时间步$t'$到$t$（$t' < t$）之间一直近似1，那么，在时间步$t'$到$t$之间的输入信息几乎没有流入时间步$t$的隐藏状态$\\boldsymbol{H}_t$。实际上，这可以看作是较早时刻的隐藏状态$\\boldsymbol{H}_{t'-1}$一直通过时间保存并传递至当前时间步$t$，时间步$t$的候选隐藏状态$\\tilde{\\boldsymbol{H}}_t \\in \\mathbb{R}^{n \\times h}$来辅助稍后的隐藏状态计算，使得模型更好地捕捉时间序列中时间步距离较大的依赖关系。\n",
    "\n",
    "\n",
    "![Image Name](https://zh.gluon.ai/_images/gru_3.svg)\n",
    "\n",
    "\n",
    "\n",
    "### 长短期记忆（LSTM）\n",
    "\n",
    "\n",
    "长短期记忆（long short-term memory，LSTM）[3] 网络，比门控循环单元的结构稍微复杂一点，它在循环神经网络的基础上引入了3个门，即输入门（input gate）、遗忘门（forget gate）和输出门（output gate），以及与隐藏状态形状相同的记忆细胞记录额外的信息。\n",
    "\n",
    "* 遗忘门控制上一时间步的记忆细胞$\\boldsymbol{C}_{t-1}$中的信息是否传递到当前时间步\n",
    "* 输入门则控制当前时间步的输入$\\boldsymbol{X}_t$通过候选记忆细胞$\\tilde{\\boldsymbol{C}}_t$如何流入当前时间步的记忆细胞\n",
    "\n",
    "\n",
    "如下图所示，LSTM可以通过元素值域在$[0, 1]$的输入门、遗忘门和输出门来控制隐藏状态中信息的流动。当前时间步记忆细胞$\\boldsymbol{C}_t \\in \\mathbb{R}^{n \\times h}$的计算组合了上一时间步记忆细胞和当前时间步候选记忆细胞的信息。有了记忆细胞以后，接下来LSTM可以通过输出门来控制从记忆细胞到隐藏状态$\\boldsymbol{H}_t \\in \\mathbb{R}^{n \\times h}$的信息的流动。\n",
    "\n",
    "\n",
    "![Image Name](https://zh.gluon.ai/_images/lstm_3.svg)\n"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "id": "4784DC942BA94F058A16200CB5167C48",
    "jupyter": {},
    "mdEditEnable": false,
    "slideshow": {
     "slide_type": "slide"
    },
    "tags": []
   },
   "source": [
    "### 问题二：\n",
    "\n",
    "- 请比较循环神经网络与门控循环网络之间结构的联系以及区别，并且从理论的角度描述门控循环神经网络的出现是在尝试解决循环神经网络中存在的哪几方面不足的。\n",
    "- 调节门控循环单元（GRU）以及长短期记忆（LSTM）的超参数以及深度，分析不同超参和深度下，训练时间、语言模型困惑度（perplexity）以及创作歌词的结果等相关指标的变化，并以表格的形式进行总结。"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "id": "0D5A14B962A2473DA5D83C6F2D5D8BEE",
    "jupyter": {},
    "mdEditEnable": false,
    "slideshow": {
     "slide_type": "slide"
    },
    "tags": []
   },
   "source": [
    "## 简单循环单元（SRU）\n",
    "\n",
    "\n",
    "在保持整个模型的设计思路不发生改变的情况下，深度学习的瓶颈往往在于计算。考虑到上述循环神经网络无法进行并行运算，因此往往需要需要大量的训练时间。\n",
    "\n",
    "简单循环单元（simple recurrent unit，SRU）[4]旨在提出和探索简单快速并更具解释性的循环神经网络，因此它在门控循环网络的结构上进行了改进。\n",
    "\n",
    "\n",
    "如下图所示，SRU的模型结构去除了遗忘门（forget gate）以及重置门（reset gate）对于上一时刻隐藏状态的依赖，以便于计算机实现并行化处理。\n",
    "\n",
    "![Image Name](https://i.imgur.com/ahILNr0.png)\n",
    "\n"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "id": "1A614081E2DF44B7920E06116E09CD88",
    "jupyter": {},
    "mdEditEnable": false,
    "slideshow": {
     "slide_type": "slide"
    },
    "tags": []
   },
   "source": [
    "### 问题三：\n",
    "\n",
    "- 通过阅读论文，试着详细分析简单循环单元（SRU）还在哪些方面进行了计算优化，尝试解决循环神经网络无法并行训练的问题，从而大大提高了训练速度。\n",
    "- 调节简单循环单元（SRU）的超参数以及深度，分析不同超参和深度下，训练时间、语言模型困惑度（perplexity）以及创作歌词的结果等相关指标的变化，并以表格的形式进行总结，并与之前的网络结构进行对比分析。"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "id": "EE8664152BD541858FFE1384D0AD6899",
    "jupyter": {},
    "mdEditEnable": false,
    "slideshow": {
     "slide_type": "slide"
    },
    "tags": []
   },
   "source": [
    "### 参考文献\n",
    "\n",
    "[1] Cho, K., Van Merriënboer, B., Bahdanau, D., & Bengio, Y. (2014). On the properties of neural machine translation: Encoder-decoder approaches. arXiv preprint arXiv:1409.1259.\n",
    "\n",
    "[2] Chung, J., Gulcehre, C., Cho, K., & Bengio, Y. (2014). Empirical evaluation of gated recurrent neural networks on sequence modeling. arXiv preprint arXiv:1412.3555.\n",
    "\n",
    "[3] Hochreiter, S., & Schmidhuber, J. (1997). Long short-term memory. Neural computation, 9(8), 1735-1780.\n",
    "\n",
    "[4] Lei, T., Zhang, Y., Wang, SI., Dai, H., & Artzi, Y. (2017). Simple recurrent units for highly parallelizable recurrence. arXiv preprint arXiv:1709.02755"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "id": "23087F823DFB48958D2461FBAF7B7EA5",
    "jupyter": {},
    "mdEditEnable": false,
    "slideshow": {
     "slide_type": "slide"
    },
    "tags": []
   },
   "source": [
    "## 项目报告\n",
    "本次大作业的终审评估以项目报告作为重要依据，开放题报告的内容和排版要求请下载文件：\n",
    "\n",
    "\n",
    "[termproject1.zip](https://boyuai.oss-cn-shanghai.aliyuncs.com/disk/YouthAI%E7%A7%8B%E5%AD%A3%E6%80%9D%E7%BB%B4%E7%8F%AD-%E4%B8%8A%E8%AF%BE%E8%A7%86%E9%A2%91/termproject1.zip)\n",
    "\n",
    "需要注意的是，文件中：\n",
    "- `termproject.pdf`提供了项目报告的内容格式要求\n",
    "- `termproject_exp.pdf`提供了项目报告的内容排版样例\n",
    "\n",
    "\n",
    "推荐使用`LaTeX`软件进行报告的撰写，相关`.tex`以及`.sty`源文件一并附于文件夹中。\n"
   ]
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.7.4"
  },
  "toc": {
   "base_numbering": 1,
   "nav_menu": {},
   "number_sections": true,
   "sideBar": true,
   "skip_h1_title": false,
   "title_cell": "Table of Contents",
   "title_sidebar": "Contents",
   "toc_cell": false,
   "toc_position": {
    "height": "calc(100% - 180px)",
    "left": "10px",
    "top": "150px",
    "width": "177.531px"
   },
   "toc_section_display": true,
   "toc_window_display": true
  }
 },
 "nbformat": 4,
 "nbformat_minor": 2
}

{
 "cells": [
  {
   "cell_type": "markdown",
   "metadata": {
    "id": "420B3C65427046218B0EEE4F1A06425C",
    "jupyter": {},
    "mdEditEnable": false,
    "slideshow": {
     "slide_type": "slide"
    },
    "tags": []
   },
   "source": [
    "# 语言模型开放题"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 1,
   "metadata": {},
   "outputs": [],
   "source": [
    "import torch, torchtext\n",
    "import math\n",
    "import time\n",
    "import random"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 2,
   "metadata": {},
   "outputs": [],
   "source": [
    "device = torch.device(\"cuda\" if torch.cuda.is_available() else \"cpu\")"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "id": "6AA2434B76F8469283BB4E3924CC54DA",
    "jupyter": {},
    "mdEditEnable": false,
    "slideshow": {
     "slide_type": "slide"
    },
    "tags": []
   },
   "source": [
    "# 数据集\n",
    "本次开放题将与课程内容保持一致，将周杰伦从第一张专辑《Jay》到第十张专辑《跨时代》中的歌词作为训练语言模型的数据集，来训练一个语言模型，并在模型训练好后，用这个模型来创作新的歌词。\n",
    "\n",
    "这里简介将此数据集转换成字符级循环神经网络及其变体所需要的输入格式的方法：\n",
    "\n",
    "### 读取数据集\n",
    "\n",
    "首先读取这个数据集，为了打印方便，我们把换行符替换成空格，并且打印前40个字符。"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 3,
   "metadata": {
    "attributes": {
     "classes": [],
     "id": "",
     "n": "20"
    },
    "id": "93634041065144A28019BB51100945BB",
    "jupyter": {},
    "scrolled": false,
    "slideshow": {
     "slide_type": "slide"
    },
    "tags": []
   },
   "outputs": [
    {
     "data": {
      "text/plain": [
       "'想要有直升机 想要和你飞到宇宙去 想要和你融化在一起 融化在宇宙里 我每天每天每'"
      ]
     },
     "execution_count": 3,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "with open('Datasets/jaychou_lyrics.txt', encoding=\"utf-8\") as f:\n",
    "        corpus_chars = f.read()\n",
    "corpus_chars = corpus_chars.replace('\\n', ' ').replace('\\r', ' ')\n",
    "\n",
    "corpus_chars[:40]"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "id": "7D030E957391442CAA0F8EB9C636F659",
    "jupyter": {},
    "mdEditEnable": false,
    "slideshow": {
     "slide_type": "slide"
    },
    "tags": []
   },
   "source": [
    "### 建立字符索引\n",
    "\n",
    "我们将每个字符映射成一个从0开始的连续整数作为索引，以方便之后的数据处理。\n",
    "\n",
    "为了得到索引，我们将数据集里所有不同字符取出来，然后将其逐一映射到索引来构造词典。\n",
    "\n",
    "接着，打印`vocab_size`，即词典中不同字符的个数，又称词典大小。\n",
    "\n",
    "之后，将训练数据集中每个字符转化为索引，并打印前20个字符及其对应的索引。\n",
    "\n",
    "建立字符索引之后我们就可以对于数据集进行操作了。"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 4,
   "metadata": {
    "attributes": {
     "classes": [],
     "id": "",
     "n": "9"
    },
    "id": "1D083F84446D4BD69139F40FAB6ECE24",
    "jupyter": {},
    "scrolled": false,
    "slideshow": {
     "slide_type": "slide"
    },
    "tags": []
   },
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "vocab_size: 2582\n",
      "chars: 想要有直升机 想要和你飞到宇宙去 想要和\n",
      "indices: [1482, 1756, 2202, 2491, 356, 630, 1901, 1482, 1756, 456, 2113, 2429, 2431, 753, 1123, 1569, 1901, 1482, 1756, 456]\n"
     ]
    }
   ],
   "source": [
    "idx_to_char = list(set(corpus_chars))\n",
    "char_to_idx = dict([(char, i) for i, char in enumerate(idx_to_char)])\n",
    "vocab_size = len(char_to_idx)\n",
    "print('vocab_size:', vocab_size)\n",
    "corpus_indices = [char_to_idx[char] for char in corpus_chars]\n",
    "sample = corpus_indices[:20]\n",
    "print('chars:', ''.join([idx_to_char[idx] for idx in sample]))\n",
    "print('indices:', sample)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "id": "2F3E9687D7D040CB8CCF4D10405C7969",
    "jupyter": {},
    "mdEditEnable": false,
    "slideshow": {
     "slide_type": "slide"
    },
    "tags": []
   },
   "source": [
    "# 评价指标\n",
    "\n",
    "在本次大作业中，我们使用困惑度（perplexity）来评价语言模型的好坏，困惑度是对交叉熵损失函数做指数运算后得到的值。\n",
    "\n",
    "* 最佳情况下，模型总是把标签类别的概率预测为1，此时困惑度为1；\n",
    "* 最坏情况下，模型总是把标签类别的概率预测为0，此时困惑度为正无穷；\n",
    "* 基线情况下，模型总是预测所有类别的概率都相同，此时困惑度为类别个数。\n",
    "\n",
    "显然，任何一个有效模型的困惑度必须小于类别个数。在语言模型中，困惑度必须小于词典大小`vocab_size`。"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 5,
   "metadata": {},
   "outputs": [],
   "source": [
    "import math\n",
    "def perplexity(l_sum, n):\n",
    "    return math.exp(l_sum / n)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "id": "5CC76AEA522E4882BC23EB584B616BE4",
    "jupyter": {},
    "mdEditEnable": false,
    "slideshow": {
     "slide_type": "slide"
    },
    "tags": []
   },
   "source": [
    "# 语言模型选取\n",
    "\n",
    "以下进入本次大作业的正题——语言模型的分析与比较。\n",
    "\n",
    "学员需要根据每个知识点介绍之后的问题进行理论分析、模型实现、结果汇总以及指标分析，尽可能完善地阐述实验收获以回答问题，并且以撰写报告的形式汇总自己的研究成果。"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## 文本预处理函数"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 6,
   "metadata": {},
   "outputs": [],
   "source": [
    "# utils\n",
    "def one_hot(x, n_class, dtype=torch.float32):\n",
    "    \"\"\"\n",
    "    input shape :  (n,)\n",
    "    output shape:  (n, n+class)\n",
    "    \"\"\"\n",
    "    result = torch.zeros(x.shape[0], n_class, dtype=dtype, device=x.device)\n",
    "    result.scatter_(1, x.long().view(-1, 1), 1)\n",
    "    return result\n",
    "\n",
    "def to_onehot(X, n_class):\n",
    "    \"\"\"\n",
    "    input shape :  (batch_size, num_steps)\n",
    "    output shape:  num_steps x (batch_size, n_class)\n",
    "    \"\"\"\n",
    "    return [one_hot(X[:, i], n_class) for i in range(X.shape[1])]\n",
    "\n",
    "def data_iter_consecutive(corpus_indices, batch_size, num_steps, device=None):\n",
    "    \"\"\"\n",
    "    X shape:   (batch_size, num_steps)\n",
    "    Y shape:   (batch_size, num_steps)\n",
    "    \"\"\"\n",
    "    if device is None:\n",
    "        device = torch.device('cuda' if torch.cuda.is_available() else 'cpu')\n",
    "    corpus_len = len(corpus_indices) // batch_size * batch_size\n",
    "    corpus_indices = corpus_indices[: corpus_len]\n",
    "    indices = torch.tensor(corpus_indices, device=device)\n",
    "    indices = indices.view(batch_size, -1)  # resize成(batch_size, )\n",
    "    batch_num = (indices.shape[1] - 1) // num_steps\n",
    "    for i in range(batch_num):\n",
    "        i = i * num_steps\n",
    "        X = indices[:, i: i + num_steps]\n",
    "        Y = indices[:, i + 1: i + num_steps + 1]\n",
    "        yield X, Y\n",
    "        \n",
    "def grad_clipping(params, theta, device=None):\n",
    "    if device is None:\n",
    "        device = torch.device('cuda' if torch.cuda.is_available() else 'cpu')\n",
    "    norm = torch.tensor([0.0], device=device)\n",
    "    for param in params:\n",
    "        norm += (param.grad.data ** 2).sum()\n",
    "    norm = norm.sqrt().item()\n",
    "    if norm > theta:\n",
    "        for param in params:\n",
    "            param.grad.data *= (theta / norm)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## RNN网络"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 7,
   "metadata": {},
   "outputs": [],
   "source": [
    "# RNN\n",
    "class RNNModel(torch.nn.Module):\n",
    "    \"\"\"\n",
    "    RNN input shape   : (num_steps, batch_size, vocab_size) if batch_first=False\n",
    "    RNN output shape  : (num_steps, batch_size, vocab_size) if batch_first=False\n",
    "    RNN hiddens shape : (num_steps, batch_size, hidden_size) if batch_first=False\n",
    "    state shape       : (num_layers * num_directions, batch_size, vocab_size)\n",
    "    dense output shape: (num_steps * batch_size, vocab_size)\n",
    "    \"\"\"\n",
    "    def __init__(self, rnn_layer):\n",
    "        super(RNNModel, self).__init__()\n",
    "        self.rnn = rnn_layer\n",
    "        self.vocab_size = rnn_layer.input_size\n",
    "        self.hidden_size = rnn_layer.hidden_size * (2 if rnn_layer.bidirectional else 1)\n",
    "        self.dense = torch.nn.Linear(self.hidden_size, vocab_size)\n",
    "        self.state = None\n",
    "\n",
    "    def forward(self, inputs, state):\n",
    "        \"\"\"\n",
    "        inputs shape: (batch_size, num_steps)\n",
    "        X shape:      num_steps x (batch_size, vocab_size)\n",
    "        hiddens:\n",
    "        \"\"\"\n",
    "        X = to_onehot(inputs, self.vocab_size)\n",
    "        X = torch.stack(X)\n",
    "        hiddens, state = self.rnn(X, state)\n",
    "        hiddens = hiddens.view(-1, hiddens.shape[-1])    # (num_steps*batch_size, hidden_size)\n",
    "        output = self.dense(hiddens)\n",
    "        return output, state"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## 预测函数"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 8,
   "metadata": {},
   "outputs": [],
   "source": [
    "def predict_rnn(prefix, num_chars, model, idx_to_char, char_to_idx, device=None):\n",
    "    \"\"\"\n",
    "    batch_size = 1\n",
    "    num_steps  = 1\n",
    "    \"\"\"\n",
    "    if device is None:\n",
    "        device = torch.device('cuda' if torch.cuda.is_available() else 'cpu')\n",
    "    model.to(device)\n",
    "    state = None\n",
    "    output = [char_to_idx[prefix[0]]]\n",
    "    for t in range(num_chars + len(prefix) - 1):\n",
    "        X = torch.tensor([output[-1]], device=device).view(1,1)\n",
    "        if state is not None:\n",
    "            if isinstance(state, tuple):  # LSTM, state:(h, c)\n",
    "                state = (state[0].to(device), state[1].to(device))\n",
    "            else:\n",
    "                state = state.to(device)\n",
    "        (Y, state) = model(X, state)\n",
    "        if t < len(prefix) - 1:\n",
    "            output.append(char_to_idx[prefix[t + 1]])\n",
    "        else:\n",
    "            output.append(Y.argmax(dim=1).item())\n",
    "    return ''.join([idx_to_char[i] for i in output])\n"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## 训练函数"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 9,
   "metadata": {},
   "outputs": [],
   "source": [
    "def train_and_predict_rnn(model, corpus_indices, idx_to_char, char_to_idx,\n",
    "                          num_steps, batch_size,\n",
    "                          num_epochs, lr, clipping_theta,\n",
    "                          pred_period, pred_len, prefixes, device=None):\n",
    "    loss = torch.nn.CrossEntropyLoss()\n",
    "    optimizer = torch.optim.Adam(model.parameters(), lr=lr)\n",
    "    model.to(device)\n",
    "    for epoch in range(num_epochs):\n",
    "        l_sum, n, start = 0.0, 0, time.time()\n",
    "        data_iter = data_iter_consecutive(corpus_indices, batch_size, num_steps, device)  # 相邻采样\n",
    "        state = None\n",
    "        for X, Y in data_iter:\n",
    "            if state is not None:\n",
    "                # 相邻采样使用detach函数从计算图分离隐藏状态\n",
    "                if isinstance(state, tuple):\n",
    "                    state[0].detach_()\n",
    "                    state[1].detach_()\n",
    "                else:\n",
    "                    state.detach_()\n",
    "            (output, state) = model(X, state)  # output.shape: (num_steps * batch_size, vocab_size)\n",
    "            y = torch.flatten(Y.T)             # Y.shape:      (batch_size, num_steps)\n",
    "            l = loss(output, y.long())         # y.shape:      (num_steps * batch_size, 1)\n",
    "\n",
    "            optimizer.zero_grad()\n",
    "            l.backward()\n",
    "            grad_clipping(model.parameters(), clipping_theta, device)\n",
    "            optimizer.step()\n",
    "            l_sum += l.item() * y.shape[0]\n",
    "            n += y.shape[0]\n",
    "\n",
    "        if (epoch + 1) % pred_period == 0:\n",
    "            print('epoch %d, perplexity %f, time %.2f sec' % (epoch + 1, math.exp(l_sum / n), time.time() - start))\n",
    "            for prefix in prefixes:\n",
    "                print(' -', predict_rnn(prefix, pred_len, model, idx_to_char, char_to_idx, device))"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## 循环神经网络\n",
    "\n",
    "在$n$元语法中，时间步$t$的词$w_t$基于前面所有词的条件概率只考虑了最近时间步的$n-1$个词。如果要考虑比$t-(n-1)$更早时间步的词对$w_t$的可能影响，模型参数的数量将随之呈指数级增长。\n",
    "\n",
    "循环神经网络（recurrent neural network，RNN）并非刚性地记忆所有固定长度的序列，而是通过隐藏状态来存储之前时间步的信息。具体来说，在循环神经网络中，时间步$t$的隐藏变量的计算由当前时间步的输入和上一时间步的隐藏变量共同决定：\n",
    "\n",
    "$${H}_t = \\phi({X}_t {W}_{xh} + {H}_{t-1} {W}_{hh}  + {b}_h).$$\n",
    "\n",
    "如下图所示，隐藏变量能够捕捉截至当前时间步的序列的历史信息，就像是神经网络当前时间步的状态或记忆一样。\n",
    "\n",
    "![Image Name](https://zh.gluon.ai/_images/rnn.svg)\n",
    "\n",
    "我们使用Pytorch中的`nn.RNN`来构造循环神经网络。在本节中，我们主要关注`nn.RNN`的以下几个构造函数参数：\n",
    "\n",
    "* `input_size` - The number of expected features in the input x\n",
    "* `hidden_size` – The number of features in the hidden state h\n",
    "* `nonlinearity` – The non-linearity to use. Can be either 'tanh' or 'relu'. Default: 'tanh'\n",
    "* `batch_first` – If True, then the input and output tensors are provided as (batch_size, num_steps, input_size). Default: False\n",
    "\n",
    "这里的`batch_first`决定了输入的形状，我们使用默认的参数`False`，对应的输入形状是 (num_steps, batch_size, input_size)。\n",
    "\n",
    "`forward`函数的参数为：\n",
    "\n",
    "* `input` of shape (num_steps, batch_size, input_size): tensor containing the features of the input sequence. \n",
    "* `h_0` of shape (num_layers * num_directions, batch_size, hidden_size): tensor containing the initial hidden state for each element in the batch. Defaults to zero if not provided. If the RNN is bidirectional, num_directions should be 2, else it should be 1.\n",
    "* `forward`函数的返回值是：\n",
    "    * `output` of shape (num_steps, batch_size, num_directions * hidden_size): tensor containing the output features (h_t) from the last layer of the RNN, for each t.\n",
    "    * `h_n` of shape (num_layers * num_directions, batch_size, hidden_size): tensor containing the hidden state for t = num_steps.\n"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "id": "4F5454D7A46144D186F770924480B436",
    "jupyter": {},
    "mdEditEnable": false,
    "slideshow": {
     "slide_type": "slide"
    },
    "tags": []
   },
   "source": [
    "### 问题一：\n",
    "- 解释为什么循环神经网络可以表达某时间步的词基于文本序列中所有过去的词的条件概率？\n",
    "- 调节循环神经网络的超参数以及深度，分析不同超参和深度下，训练时间、语言模型困惑度（perplexity）以及创作歌词的结果等相关指标的变化，并以表格的形式进行总结。"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "#### RNN可表达过去词的条件概率\n",
    "循环神经网络引入了隐藏变量$H_t$，`RNN`神经网络的输入包括代表历史信息的$H_{t-1}$和代表本时刻的$X_t$，$H_t$又可以作为下一时刻的输入，因此$H_t$能够捕捉截至当前时间步序列的历史信息。"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### RNN模型的超参数\n",
    "- bidirectional\n",
    "- hidden_size\n",
    "- num_layers"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 10,
   "metadata": {},
   "outputs": [],
   "source": [
    "num_steps, batch_size = 35, 32\n",
    "num_epochs, lr, clipping_theta = 250, 1e-3, 1e-2\n",
    "pred_period, pred_len, prefixes = 50, 100, ['分开', '不分开']"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "#### bidirectional\n",
    "本体不适合bidirectional"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 11,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "epoch 50, perplexity 1.004757, time 1.43 sec\n",
      " - 分开分开分开分开分开分开分开分开分开分开分开分开分开分开分开分开分开分开分开分开分开分开分开分开分开分开分开分开分开分开分开分开分开分开分开分开分开分开分开分开分开分开分开分开分开分开分开分开分开分开分开\n",
      " - 不分开分开分开分开分开分开分开分开分开分开分开分开分开分开分开分开分开分开分开分开分开分开分开分开分开分开分开分开分开分开分开分开分开分开分开分开分开分开分开分开分开分开分开分开分开分开分开分开分开分开分开\n",
      "epoch 100, perplexity 1.000438, time 1.43 sec\n",
      " - 分开分开分开分开分开分开分开分开分开分开分开分开分开分开分开分开分开分开分开分开分开分开分开分开分开分开分开分开分开分开分开分开分开分开分开分开分开分开分开分开分开分开分开分开分开分开分开分开分开分开分开\n",
      " - 不分开分开分开分开分开分开分开分开分开分开分开分开分开分开分开分开分开分开分开分开分开分开分开分开分开分开分开分开分开分开分开分开分开分开分开分开分开分开分开分开分开分开分开分开分开分开分开分开分开分开分开\n",
      "epoch 150, perplexity 1.000089, time 1.39 sec\n",
      " - 分开分开分开分开分开分开分开分开分开分开分开分开分开分开分开分开分开分开分开分开分开分开分开分开分开分开分开分开分开分开分开分开分开分开分开分开分开分开分开分开分开分开分开分开分开分开分开分开分开分开分开\n",
      " - 不分开分开分开分开分开分开分开分开分开分开分开分开分开分开分开分开分开分开分开分开分开分开分开分开分开分开分开分开分开分开分开分开分开分开分开分开分开分开分开分开分开分开分开分开分开分开分开分开分开分开分开\n",
      "epoch 200, perplexity 1.019067, time 1.51 sec\n",
      " - 分开分开分开分开分开分开分开分开分开分开分开分开分开分开分开分开分开分开分开分开分开分开分开分开分开分开分开分开分开分开分开分开分开分开分开分开分开分开分开分开分开分开分开分开分开分开分开分开分开分开分开\n",
      " - 不分开分开分开分开分开分开分开分开分开分开分开分开分开分开分开分开分开分开分开分开分开分开分开分开分开分开分开分开分开分开分开分开分开分开分开分开分开分开分开分开分开分开分开分开分开分开分开分开分开分开分开\n",
      "epoch 250, perplexity 1.000124, time 1.45 sec\n",
      " - 分开分开分开分开分开分开分开分开分开分开分开分开分开分开分开分开分开分开分开分开分开分开分开分开分开分开分开分开分开分开分开分开分开分开分开分开分开分开分开分开分开分开分开分开分开分开分开分开分开分开分开\n",
      " - 不分开分开分开分开分开分开分开分开分开分开分开分开分开分开分开分开分开分开分开分开分开分开分开分开分开分开分开分开分开分开分开分开分开分开分开分开分开分开分开分开分开分开分开分开分开分开分开分开分开分开分开\n"
     ]
    }
   ],
   "source": [
    "num_hiddens = 256\n",
    "rnn_layer = torch.nn.RNN(input_size=vocab_size, hidden_size=num_hiddens, num_layers=1, bidirectional=True)\n",
    "model = RNNModel(rnn_layer).to(device)\n",
    "train_and_predict_rnn(model, corpus_indices, idx_to_char, char_to_idx,\n",
    "                          num_steps, batch_size,\n",
    "                          num_epochs, lr, clipping_theta,\n",
    "                          pred_period, pred_len, prefixes, device)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "#### hidden_size"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 12,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "==================== num_hiddens =  64 ====================\n",
      "epoch 50, perplexity 17.574493, time 0.78 sec\n",
      " - 分开始可以 我用麻烦了 不用麻烦了 我们一直在我的时间 我们的世界 竟然先对我示了 不该开始在飘移青春  我的眼泪 不要再想一口 他们的很美 你说不是我们的招式  我们在等待 让我在等待英雄 我 不是你的\n",
      " - 不分开 我不是我的手 你不该 我不能再想你 我 不用麻烦了 我们一直人在一点 有人是一种味道叫做家 无法伪造　我们都有了空 我用力气　我们都会怕你 我不能够语气 她的睫毛弯的嘴角 让我们的感觉 一直走到  \n",
      "epoch 100, perplexity 5.250626, time 0.85 sec\n",
      " - 分开苏美丽的心愿望人生活  你穿上 生死  无法 我在街上的风月 是否院子有空 在我无法 怀里 不用麻痹了心 在我边上 我们之前 我们指放松 我目光如龙 当敌人是空 我左右开弓 我气势如虹 将炮马尽用 兵\n",
      " - 不分开始不能 让我 想我拿出手机有了 我用眼泪想你 虽然我的我们都没有你 想想有没有结果  不要吵  难预测 我 何著你 别人爱的不 我不需要被崇拜 我们中了承诺 缘分开始如此应堪  石板上 低头可以 这个\n",
      "epoch 150, perplexity 3.294741, time 0.78 sec\n",
      " - 分开苏美丽的秘密 玻璃的出车 无都会穿你马的走 载 那风吹在淡路 古果我自己的裁判 不该犯法移不离开始没有名和装 如果只穿上窗外的老外套　 只让我们数着紧 就想你的手机 我不是孤獨 小丑 你的影 我用古堡\n",
      " - 不分开大人相想只能承受回头 所有你可以不用麻烦了 不用麻烦了 不用麻烦了 不用麻烦了 副歌不长大家米的地上 象山青铜入侵略 我睡法绝影步两步三步四 在等待  在狂风吹 后悔在你人会的画  我用第一人称时间染\n",
      "epoch 200, perplexity 2.536493, time 0.82 sec\n",
      " - 分开苏美将被摧毁 有夜点伤 把手被精准出来 这么过  我不了我 爱你不懂 你甘睡不 这个人生活不 这着了 超可了 我好了 让法吸明白在那些完全世落地对白色的帝外 将我还以开 分数必须 再来 强 如果你们有\n",
      " - 不分开大家能看说去那颜色 这个终就是人的世界 谁都是我的天 啊 这么车灯 不安跳来我的笑　 有果只让我留不及开始 我想你说没有意图 却惦记着你 我感动 爱你的爱爱 我的咖啡不想回 我的家枪很有奇怪的话常美 \n",
      "epoch 250, perplexity 2.158441, time 0.79 sec\n",
      " - 分开苏美将被摧毁 在灵魂 翻开墙上代风飘　 预命 起他们都难毛卑微的安跳 街角的天空 毁却不声太糟 要 我带着孤寂 继续前进 直到尽去 爱情走 我会乐有时代 太薄海 被别人穿越 梦就连祷告成仁慈的羞像 条\n",
      " - 不分开大家能看说装起文字 远学的开 我也能不能再重要 消息 一个人是 何想刚狠残 的晨露沾湿划的战约分化的声音 用南极的冰 将爱结晶 我用心 不急等待 香 如果只是卑微的小丑 我念了 江湖 来之前暴 随时准\n",
      "==================== num_hiddens =  128 ====================\n",
      "epoch 50, perplexity 5.278020, time 0.94 sec\n",
      " - 分开都不该 不要我不要我去 我让我没有预兆 那骄傲傲在我听 你一句 轻轻地球 她来心里不了 為你的声音乐 让我们乘着阳光　 海上冲浪　吸引她目光 不想挟带出手 雷些 於落如雪 凄美了离开 你发如雪 纷飞了\n",
      " - 不分开 为什么这样子 你拉上 你说的手机  我想要你想念你身边 等 你的微笑得吃 我的在我琴 我在等你 一点稚气不想 你说了爱你  我们都没有错 只是我心碎没有 我只能没有预兆 那些事 在山路上一路开 天我\n",
      "epoch 100, perplexity 1.769041, time 0.89 sec\n",
      " - 分开 这样的美像多难 我就必须要走 我还习惯不被强紧紧地尝一步一步四步 著天我的掌 这一颗心里的知道 你说了再孤开 有常话不重 就这样 我说不见你 他会有点让 面大地离开 印地安老斑鸠 腿短毛不多 几句抱\n",
      " - 不分开 哎哟哎哟 喔微笑 你笑我的音乐是初恋 颜色我把小村庄 梦看配去操纵 我右拳打开了天 化身为龙 把山河重新移动 填平裂缝 将东方 的日出调整了时空 回到洪荒 去支配去操纵 我右拳打开了天 化身为龙 那\n",
      "epoch 150, perplexity 1.260547, time 0.87 sec\n",
      " - 分开 这样的脆弱 把后 用南情抱女生命就咆哮哼我用这幸福现 我好想 你想回到凌哪里 不再微笑 真的好听 找不到哪的旧清 这么看 分手节拍了重  就是开没有意　 有做事会有太多事来到底 会分手说不知来 对我\n",
      " - 不分开 哎哟哎哟 喔哎哟哎哟喔喔 哎哟 不错口重新编织 脑海中起跑 半终地 不知要眼睛睡着表 只是别人的眼泪 原来不对 我要走 我不在我不 我 你的微笑像个画面 心里的雨天 许流我的很难道 我会开始学致 的\n",
      "epoch 200, perplexity 1.112252, time 0.86 sec\n",
      " - 分开 不让我们之名 我说你已无法可爱 我却不懂不为什么 我却快疯飞　别再轻我很有 我们抱会不会就会懂吗 不要问 我给你的我 大到最美的没有这样的生活 给我的悲 因为习惯 去 如兽力 的甜蜜说你手  手没有\n",
      " - 不分开 哎哟哎哟 开偷偷偷 天开就是会没有 不能说这是我说老师出 不是你们挑痛  一夕阳飞翔断了续唱 她你的伤心天气 想打球下大雨照跳　只收感动落出现不该 的世界将你们在里 秋天边的过 而我的狗　 我要我叫\n",
      "epoch 250, perplexity 1.067712, time 0.89 sec\n",
      " - 分开 不让我们之都 我已经身为我 上我的泪 手机穿重重带背 镜头发出话 心碎太绕 我真在  我根本 抽离开 阳不双截棍 右转动 真故 别人在门　 大有个性一每天在晃吉回  你就要离开 说你 断了的弦 再怎\n",
      " - 不分开 哎哟哎哟 喔…… 嘿 这里 都可以随 这样失回到 我的期待不知道  杵在伊斯坦堡 却只想你和汉堡 我想要你的微笑每天都能看到  我知道这里很美但家乡的你更美 沙漠之中怎么会有泥鳅 话说完飞过一只海鸥\n",
      "==================== num_hiddens =  256 ====================\n",
      "epoch 50, perplexity 1.965188, time 1.06 sec\n",
      " - 分开始爱也 为别人会续会走吗 你说我不想拆 你用眼神的手  就连分手都要大人 经过的美丽 别发那影子如感动的风 灵魂 翻滚 停止忿恨 永无止尽的战争 让我们 半兽人 的灵魂 单纯 对远古存在的神 用谦卑的\n",
      " - 不分开 爱分手这个钟　 有这个太多很会要我说你 请我舍不得很重 我右拳打开了天 化身为龙 那大地心脏汹涌 不安跳动 全世界 的表情只剩下一种 等待英雄 我就是那条龙 渴望着血脉相通 无限个千万弟兄 我把天地\n",
      "epoch 100, perplexity 1.078932, time 1.08 sec\n",
      " - 分开 为什么不开 为什么不连 让那年  得有一点的风 从前天中的这样多 我还是那年好 乡里 是不用心机 不过用此 还是很年  点心里继续往前走 为什么接 我对着这话 你也要靠着我不能再说 我不能体放  你\n",
      " - 不分开 为什么这个简单你做不到 仔细想这种生活安详 为什么这种速度你追不到 不好笑不好笑不好笑…… 这第一名到底要多强 不用问 别人有点绿  故事中告诉我 然用手中国风 那坚持的边不起太阳光 不要再牵着你的\n",
      "epoch 150, perplexity 1.042480, time 1.08 sec\n",
      " - 分开 为什么还是我的感性 一步两步三步四步望著天 看星星 一颗两颗三颗四颗 连成线背著背默默许下心愿 看远方的星如果听的见 它一定实现 它一定实现 娘子 娘子却依旧每日 折一枝杨柳 你在那里 在小村外的溪\n",
      " - 不分开 为什么这么简单你做不到 仔细想这种生活安详 为什么这种速度你追不到 不好笑不好笑不好笑…… 这第一名到底要多强 不用问 一定有人向你挑战 到底还要过多少关 不用怕 告诉他们谁是男子汉 可不可以不要这\n",
      "epoch 200, perplexity 1.080347, time 1.08 sec\n",
      " - 分开 为什么状  为什么 干什么 干什么 干什么 已经不是我 等你永远 不能不拿出地 表情易我可以那生是 这个决定 透明的 我在等待 说你陪 我自己的白  得得可是几个光 就能存在 是这个上节奏在正服 一\n",
      " - 不分开 为什么这么简单 做不到 坐车厢朝着南下方向 为什么这种速度你追不到 鸟飞翔穿过这条小巷 为什么这么简单你做不到 仔细想这种生活安详 为什么这种速度你追不到 不要的这样 我不懂 你说的眼泪 在正傻当一\n",
      "epoch 250, perplexity 1.018976, time 1.05 sec\n",
      " - 分开 为什么证明我没有办法模拟飘飘 得飘得飘得咿的飘   得飘得飘得咿的飘   我绕过山腰雨声敲敲   我绕过山腰雨声敲敲   得飘得飘得咿的飘   得飘得飘得咿的飘   再开进隧道风声潇潇   再开进隧\n",
      " - 不分开 为什么我有去 你看不到 原来爱跟你的背 只是你再见 手说的为敌人是我只要一直代情来 加速和画 只是我枪  得听说不要听 她问我Coffee tea or me 我张大眼睛在怀疑 厚嘴唇狠有吸引力 我\n",
      "==================== num_hiddens =  512 ====================\n",
      "epoch 50, perplexity 1.209647, time 0.77 sec\n",
      " - 分开始不要离为我 得别人爱 装长大 我爱跟了永远  开始打球下暗 互相残杀气动过了 情用无让我爱  温暖的食谱 她心的面 有空就马 我的英气 我说我不该 睁不住 不到 这里 所有　在每天决斗 到一直走 有\n",
      " - 不分开 为什么我爸爸头白的 留着口袋里有糖果刺刀的从我 经过老伯的家庭篮 而时间到了到底 我不能就想回头 你要走是我受伤 难过 只剩下一天 睡着你的想 你说完 一直走 我想就这样牵着你的手经过往上欢 你走下\n",
      "epoch 100, perplexity 1.061048, time 0.79 sec\n",
      " - 分开心打开了爱情绪激动 一颗两颗三颗四颗 连成线背著背默默许下心愿 看远方的星如果听的见 它一定实现 它一定实现 娘子 娘子却依旧每日 折一枝杨柳 你在那里 在小村外的溪边河口默默等著我 娘子依旧每日折一\n",
      " - 不分开 为什么这样子们都有个人说说不要逃跑 不知道 我没有孤寂　继续沉事 和在山腰间过雾 多远方龙 那野声行 汹涌失控 城市霓虹 不安跳动 染红夜空 过去种种 象一场梦 不敢去碰 一想就痛留心碎内容 每一秒\n",
      "epoch 150, perplexity 1.024909, time 0.78 sec\n",
      " - 分开 为什么还要我用微笑来带过 我没有这种天份 包容你也接受他 不用担心的太多 我会一直好好过 你已经远远离开 我也会慢慢走开 为什么我连分开都迁就着你 我真的没有天份 安静的没这么快 我会学着放弃你 是\n",
      " - 不分开 为什么这样子 你拉着我 说你有些犹豫 怎么这样子 雨还没停你就撑伞要走 已经习惯不去阻止你 过好一阵子你就会回来 印象中的爱情好像顶不住那时间 为什么这样子 你拉着我 说你有些犹豫 怎么这样子 雨还\n",
      "epoch 200, perplexity 1.022773, time 0.79 sec\n",
      " - 分开心 一路乘着溜滑梯 我们都会走到 我就必须要带 珍贵 我 我们 雨淋湿的天空 心送你想是 我用手机一点 那只是我在的手  轻轻的地   也没有纯白的灵魂 自人类堕落为半兽人 我开始使用第一人称 记录眼\n",
      " - 不分开 为什么我 跑得比别人快 飞得比别人高 将来大家看的都是我画的漫画 大家唱的都是 我写的歌 妈妈的辛苦 不让你看见 温暖的食谱在她心里面 有空就多多握握她的手 把手牵着一起梦游 听妈妈的话 别让她受伤\n",
      "epoch 250, perplexity 1.023821, time 0.77 sec\n",
      " - 分开心能不能够继续爱我 我用力牵起没温度的双手 过往温柔 已经被时间上锁 只剩挥散不去的难过 在山腰间飘逸的红雨 随着北风凋零 我轻轻摇曳风铃 想 唤醒被遗弃的爱情 雪花已铺满了地 深怕窗外枫叶已结成冰 \n",
      " - 不分开 为什么这样子 你拉着我 说你有些犹豫 怎么这样子 雨还没停你就撑伞要走 已经习惯不去阻止你 过好一阵子你就会回来 印象中的爱情好像顶不住那时间 为什么这样子 你拉着我 说你有些犹豫 怎么这样子 雨还\n"
     ]
    }
   ],
   "source": [
    "hidden_size_seq = [64, 128, 256, 512]\n",
    "for num_hiddens in hidden_size_seq:\n",
    "    print(\"=\"*20, \"num_hiddens = \", num_hiddens, \"=\"*20)\n",
    "    rnn_layer = torch.nn.RNN(input_size=vocab_size, hidden_size=num_hiddens, num_layers=1, bidirectional=False)\n",
    "    model = RNNModel(rnn_layer).to(device)\n",
    "    train_and_predict_rnn(model, corpus_indices, idx_to_char, char_to_idx,\n",
    "                          num_steps, batch_size,\n",
    "                          num_epochs, lr, clipping_theta,\n",
    "                          pred_period, pred_len, prefixes, device)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "#### num_layers"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "==================== num_layer =  1 ====================\n",
      "epoch 50, perplexity 1.173704, time 1.39 sec\n",
      " - 分开 不需要勇气 我也不够不 听不到我才能够想要没有错过的方式 你在我的世界 爱从中穿越 梦与希望在飞 我向前去追 有目标就无悔 等着我超越 梦想挟带眼泪 咸咸的汉水 你我同个世界 爱从中穿越 梦与希望在\n",
      " - 不分开 寒风中尘埃 你 今 是外婆家是 不对我 你在爱我 我知道 你没有了 说 有些事关  让我有几个机  你要的很快乐开 祝想你离开 装不能不能够分辨 边 你用水的一出天气 第一个纸伞 大自己错人在吧 这\n",
      "epoch 100, perplexity 1.065370, time 1.28 sec\n",
      " - 分开 不让我受伤 想快快长大 才能保护她 长大后我开始明白 为什么我 跑得比别人快 飞得比别人高 将来大家看的都是我画的漫画 大家唱的都是 我写的歌 妈妈的辛苦 不让你看见 温暖的食谱在她心里面 有空就多\n",
      " - 不分开 爱让我猜不得我 本真的狠难过 方向你看不画的手 那恐惧刻的孩子们 一起手牵手 温度的微笑 天涯灰宫 已着我心碎 清晰 原来最美丽的地下 我轻轻真的的模样 微如绝 分手在苦痛 就来难过 在伤透 心能看\n",
      "epoch 150, perplexity 1.036923, time 1.30 sec\n",
      " - 分开 不让我颠倒 活着 话不不对 我不能为你的我 你不懂 说不出  海分手说不出来 蔚蓝的珊瑚海 错过瞬间苍白 当初彼此 你我都  不够成熟坦白  不应该  热情不再  你的 笑容勉强不来 爱深埋珊瑚海 \n",
      " - 不分开 爱让我猜不是我不该 不该在这时候说了我爱你 要怎么证明我没有说谎力气 请告诉我暂停算不算放弃 我只有一天的回忆 能不能给我一首歌的时间 紧紧的把那拥抱变成永远 在我的怀里你不用害怕失眠 哦 如果你想\n",
      "epoch 200, perplexity 1.023478, time 1.31 sec\n",
      " - 分开 爱让我还说你 谁都了我不能 能够翻阅 的到这么我的样 我妈妈 我说爱 你要离开我知道很简单　 你说依赖是我们的阻碍 就算放开但能不能别没收我的爱　 当作我最后才明白 青 花 瓷 素胚勾勒出青花 笔锋\n",
      " - 不分开 爱所有些爱情 在我的眼泪 看 那些 如果我遇见你是一场悲剧 我想我这辈子注定一个人演戏 最后再一个人慢慢的回忆 没有了过去 我将往事抽离 如果我遇见你是一场悲剧 我轻轻的叹息 后悔着对不起 藤蔓植物\n",
      "epoch 250, perplexity 1.022006, time 1.34 sec\n",
      " - 分开 为什么还要我用微笑来带过 我没有这种天份 包容你也接受他 不用担心的太多 我会一直好好过 你已经远远离开 我也会慢慢走开 为什么我连分开都迁就着你 我真的没有天份 安静的没这么快 我会学着放弃你 是\n",
      " - 不分开 爱所有的爱情 你听得出为我 等雨变 我理解的游戏 给我们 伽蓝寺听雨声盼 永恒 喂 老板  哎 年轻人 又来看吉他啦  我要那把  那把 啊 哈哈哈 那把是琵琶 不是吉他  我说旁边那把  旁边那把\n",
      "==================== num_layer =  2 ====================\n",
      "epoch 50, perplexity 341.909764, time 1.65 sec\n",
      " - 分开 我我  我我 我 我 我 我 我 我 我 我 我 我 我 我 我 我 我 我 我 我 我 我 我 我 我 我 我 我 我 我 我 我 我 我 我 我 我 我 我 我 我 我 我 我 我 我 我 我 \n",
      " - 不分开 我  我我 我 我 我 我 我 我 我 我 我 我 我 我 我 我 我 我 我 我 我 我 我 我 我 我 我 我 我 我 我 我 我 我 我 我 我 我 我 我 我 我 我 我 我 我 我 我 我\n",
      "epoch 100, perplexity 209.375630, time 1.74 sec\n",
      " - 分开 我用人人人 我们我我的人了 我的人人人 我的人人不人 我不人的人去 我们我不人的人  我的人人人 我的人人不人 我不人的人去 我们我不人的人  我的人人人 我的人人不人 我不人的人去 我们我不人的人\n",
      " - 不分开 我的人人去 我的人人不人 我不人的人了 我的人不人 我的人人的人  我不人的人  我不人的人  我不人的人  我不人的人  我不人的人  我不人的人  我不人的人  我不人的人  我不人的人  我不\n",
      "epoch 150, perplexity 149.693607, time 1.70 sec\n",
      " - 分开 我不开的我 我的人来 我我的不个手的我不了 我的不人我的人不开了 我不人我在我不来 我的人一人的手来 我不人你我 我用不个个人 我你的手去的不个 我在我不人 我的着一人的人不的人你的手的我我的我一人\n",
      " - 不分开的人了 我不不了不个手 我不不来 我你不人 我不来 我我不个人一人不个 我的不人 我不个人一人的人不不开 我不人不人 我不不个人我的来的我我的手了我的开的我在我不人 我不手的我的开的我一人不人 我不人\n",
      "epoch 200, perplexity 129.803863, time 1.65 sec\n",
      " - 分开 用个个么烦 我不人烦了 你的个么一个个人  你你一么的伤  我不一么烦了 在我一么个烦来 你你不一么 你你 我不烦了 你的么一么 你的来c烦 我一么一么一么一么 你的人一么的风 你你在的么么烦 我不\n",
      " - 不分开的用烦 我来你你你的伤 你不烦了 一么一么烦 我不人人的人 你的个么一么c个 你的人人一么 你在用烦了 我的人一么 我一么人一么c烦  人一么一么 你你的烦来 你的么一么 我一么一么 你你的一么 我不\n",
      "epoch 250, perplexity 101.116691, time 1.83 sec\n",
      " - 分开 我不当烦去 我在用去一人一人 你在在用个烦了 在我不在一人的你的你的你不在你在我不过了 你在在人来的烦　 我在我不人 你在我不是你在你 我不去的人在我的人一个个烦上 我不光　你的风光了过 你一人 我\n",
      " - 不分开在我不在我个一人不开  我不人一人的风过 我在人的风了白来 我不情情来 你用个烦了 去在用个人 你在你不人 我不去的人的你的开你 我在人来在我的人在我的风烦气 我不一人不在你的我不去 我不用烦子 你不\n",
      "==================== num_layer =  3 ====================\n",
      "epoch 50, perplexity 440.327324, time 2.16 sec\n",
      " - 分开有                                                                                                   \n",
      " - 不分开                                                                                                    \n",
      "epoch 100, perplexity 439.129878, time 2.06 sec\n",
      " - 分开别一                                                                                                  \n",
      " - 不分开                                                                                                    \n",
      "epoch 150, perplexity 441.960942, time 3.07 sec\n",
      " - 分开有要                                                                                                  \n",
      " - 不分开要                                                                                                   \n",
      "epoch 200, perplexity 348.448299, time 3.22 sec\n",
      " - 分开不琴升   我  让     飞  气  飞  我  飞  我  飞  我  飞  我  飞  我  飞  我  飞  我  飞  我  飞  我  飞  我  飞  我  飞  我  飞  我  飞\n",
      " - 不分开要回   气  飞  我  飞  我  飞  我  飞  我  飞  我  飞  我  飞  我  飞  我  飞  我  飞  我  飞  我  飞  我  飞  我  飞  我  飞  我  飞 \n"
     ]
    }
   ],
   "source": [
    "num_layers_seq = [1, 2, 3]\n",
    "for num_layer in num_layers_seq:\n",
    "    print(\"=\" * 20, \"num_layer = \", num_layer, \"=\" * 20)\n",
    "    rnn_layer = torch.nn.RNN(input_size=vocab_size, hidden_size=256, num_layers=num_layer, bidirectional=False)\n",
    "    model = RNNModel(rnn_layer).to(device)\n",
    "    train_and_predict_rnn(model, corpus_indices, idx_to_char, char_to_idx,\n",
    "                          num_steps, batch_size,\n",
    "                          num_epochs, lr, clipping_theta,\n",
    "                          pred_period, pred_len, prefixes, device)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "id": "CC32E514DB82434B80891531DD242AA7",
    "jupyter": {},
    "mdEditEnable": false,
    "slideshow": {
     "slide_type": "slide"
    },
    "tags": []
   },
   "source": [
    "## 门控循环神经网络\n",
    "\n",
    "\n",
    "门控循环神经网络（gated recurrent neural network）通过可以学习的门来控制信息的流动，可以更好地捕捉时间序列中时间步距离较大的依赖关系。\n",
    "\n",
    "\n",
    "### 门控循环单元（GRU）\n",
    "\n",
    "\n",
    "\n",
    "\n",
    "\n",
    "门控循环单元（gated recurrent unit，GRU）是一种常用的门控循环神经网络[1, 2]，它引入了重置门（reset gate）和更新门（update gate）的概念，并且计算候选隐藏状态，从而修改了循环神经网络中隐藏状态的计算方式。\n",
    "\n",
    "* 重置门控制了上一时间步的隐藏状态如何流入当前时间步的候选隐藏状态，有助于捕捉时间序列里短期的依赖关系；\n",
    "* 更新门可以控制隐藏状态应该如何被包含当前时间步信息的候选隐藏状态所更新，有助于捕捉时间序列里长期的依赖关系。\n",
    "\n",
    "如下图所示，假设更新门在时间步$t'$到$t$（$t' < t$）之间一直近似1，那么，在时间步$t'$到$t$之间的输入信息几乎没有流入时间步$t$的隐藏状态$\\boldsymbol{H}_t$。实际上，这可以看作是较早时刻的隐藏状态$\\boldsymbol{H}_{t'-1}$一直通过时间保存并传递至当前时间步$t$，时间步$t$的候选隐藏状态$\\tilde{\\boldsymbol{H}}_t \\in \\mathbb{R}^{n \\times h}$来辅助稍后的隐藏状态计算，使得模型更好地捕捉时间序列中时间步距离较大的依赖关系。\n",
    "\n",
    "\n",
    "![Image Name](https://zh.gluon.ai/_images/gru_3.svg)\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "hidden_size_seq = [64, 128, 256, 512]\n",
    "for num_hiddens in hidden_size_seq:\n",
    "    print(\"=\"*20, \"num_hiddens = \", num_hiddens, \"=\"*20)\n",
    "    rnn_layer = torch.nn.GRU(input_size=vocab_size, hidden_size=num_hiddens, num_layers=1, bidirectional=False)\n",
    "    model = RNNModel(rnn_layer).to(device)\n",
    "    train_and_predict_rnn(model, corpus_indices, idx_to_char, char_to_idx,\n",
    "                          num_steps, batch_size,\n",
    "                          num_epochs, lr, clipping_theta,\n",
    "                          pred_period, pred_len, prefixes, device)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### 长短期记忆（LSTM）\n",
    "\n",
    "\n",
    "长短期记忆（long short-term memory，LSTM）[3] 网络，比门控循环单元的结构稍微复杂一点，它在循环神经网络的基础上引入了3个门，即输入门（input gate）、遗忘门（forget gate）和输出门（output gate），以及与隐藏状态形状相同的记忆细胞记录额外的信息。\n",
    "\n",
    "* 遗忘门控制上一时间步的记忆细胞$\\boldsymbol{C}_{t-1}$中的信息是否传递到当前时间步\n",
    "* 输入门则控制当前时间步的输入$\\boldsymbol{X}_t$通过候选记忆细胞$\\tilde{\\boldsymbol{C}}_t$如何流入当前时间步的记忆细胞\n",
    "\n",
    "\n",
    "如下图所示，LSTM可以通过元素值域在$[0, 1]$的输入门、遗忘门和输出门来控制隐藏状态中信息的流动。当前时间步记忆细胞$\\boldsymbol{C}_t \\in \\mathbb{R}^{n \\times h}$的计算组合了上一时间步记忆细胞和当前时间步候选记忆细胞的信息。有了记忆细胞以后，接下来LSTM可以通过输出门来控制从记忆细胞到隐藏状态$\\boldsymbol{H}_t \\in \\mathbb{R}^{n \\times h}$的信息的流动。\n",
    "\n",
    "\n",
    "![Image Name](https://zh.gluon.ai/_images/lstm_3.svg)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "id": "4784DC942BA94F058A16200CB5167C48",
    "jupyter": {},
    "mdEditEnable": false,
    "slideshow": {
     "slide_type": "slide"
    },
    "tags": []
   },
   "source": [
    "### 问题二：\n",
    "\n",
    "- 请比较循环神经网络与门控循环网络之间结构的联系以及区别，并且从理论的角度描述门控循环神经网络的出现是在尝试解决循环神经网络中存在的哪几方面不足的。\n",
    "- 调节门控循环单元（GRU）以及长短期记忆（LSTM）的超参数以及深度，分析不同超参和深度下，训练时间、语言模型困惑度（perplexity）以及创作歌词的结果等相关指标的变化，并以表格的形式进行总结。"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "id": "0D5A14B962A2473DA5D83C6F2D5D8BEE",
    "jupyter": {},
    "mdEditEnable": false,
    "slideshow": {
     "slide_type": "slide"
    },
    "tags": []
   },
   "source": [
    "## 简单循环单元（SRU）\n",
    "\n",
    "\n",
    "在保持整个模型的设计思路不发生改变的情况下，深度学习的瓶颈往往在于计算。考虑到上述循环神经网络无法进行并行运算，因此往往需要需要大量的训练时间。\n",
    "\n",
    "简单循环单元（simple recurrent unit，SRU）[4]旨在提出和探索简单快速并更具解释性的循环神经网络，因此它在门控循环网络的结构上进行了改进。\n",
    "\n",
    "\n",
    "如下图所示，SRU的模型结构去除了遗忘门（forget gate）以及重置门（reset gate）对于上一时刻隐藏状态的依赖，以便于计算机实现并行化处理。\n",
    "\n",
    "![Image Name](https://i.imgur.com/ahILNr0.png)\n",
    "\n"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "id": "1A614081E2DF44B7920E06116E09CD88",
    "jupyter": {},
    "mdEditEnable": false,
    "slideshow": {
     "slide_type": "slide"
    },
    "tags": []
   },
   "source": [
    "### 问题三：\n",
    "\n",
    "- 通过阅读论文，试着详细分析简单循环单元（SRU）还在哪些方面进行了计算优化，尝试解决循环神经网络无法并行训练的问题，从而大大提高了训练速度。\n",
    "- 调节简单循环单元（SRU）的超参数以及深度，分析不同超参和深度下，训练时间、语言模型困惑度（perplexity）以及创作歌词的结果等相关指标的变化，并以表格的形式进行总结，并与之前的网络结构进行对比分析。"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "id": "EE8664152BD541858FFE1384D0AD6899",
    "jupyter": {},
    "mdEditEnable": false,
    "slideshow": {
     "slide_type": "slide"
    },
    "tags": []
   },
   "source": [
    "### 参考文献\n",
    "\n",
    "[1] Cho, K., Van Merriënboer, B., Bahdanau, D., & Bengio, Y. (2014). On the properties of neural machine translation: Encoder-decoder approaches. arXiv preprint arXiv:1409.1259.\n",
    "\n",
    "[2] Chung, J., Gulcehre, C., Cho, K., & Bengio, Y. (2014). Empirical evaluation of gated recurrent neural networks on sequence modeling. arXiv preprint arXiv:1412.3555.\n",
    "\n",
    "[3] Hochreiter, S., & Schmidhuber, J. (1997). Long short-term memory. Neural computation, 9(8), 1735-1780.\n",
    "\n",
    "[4] Lei, T., Zhang, Y., Wang, SI., Dai, H., & Artzi, Y. (2017). Simple recurrent units for highly parallelizable recurrence. arXiv preprint arXiv:1709.02755"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "id": "23087F823DFB48958D2461FBAF7B7EA5",
    "jupyter": {},
    "mdEditEnable": false,
    "slideshow": {
     "slide_type": "slide"
    },
    "tags": []
   },
   "source": [
    "## 项目报告\n",
    "本次大作业的终审评估以项目报告作为重要依据，开放题报告的内容和排版要求请下载文件：\n",
    "\n",
    "\n",
    "[termproject1.zip](https://boyuai.oss-cn-shanghai.aliyuncs.com/disk/YouthAI%E7%A7%8B%E5%AD%A3%E6%80%9D%E7%BB%B4%E7%8F%AD-%E4%B8%8A%E8%AF%BE%E8%A7%86%E9%A2%91/termproject1.zip)\n",
    "\n",
    "需要注意的是，文件中：\n",
    "- `termproject.pdf`提供了项目报告的内容格式要求\n",
    "- `termproject_exp.pdf`提供了项目报告的内容排版样例\n",
    "\n",
    "\n",
    "推荐使用`LaTeX`软件进行报告的撰写，相关`.tex`以及`.sty`源文件一并附于文件夹中。\n"
   ]
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.7.4"
  },
  "toc": {
   "base_numbering": 1,
   "nav_menu": {},
   "number_sections": true,
   "sideBar": true,
   "skip_h1_title": false,
   "title_cell": "Table of Contents",
   "title_sidebar": "Contents",
   "toc_cell": false,
   "toc_position": {
    "height": "calc(100% - 180px)",
    "left": "10px",
    "top": "150px",
    "width": "345.188px"
   },
   "toc_section_display": true,
   "toc_window_display": true
  }
 },
 "nbformat": 4,
 "nbformat_minor": 2
}

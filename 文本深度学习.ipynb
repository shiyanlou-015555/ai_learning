{
 "cells": [
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# 文本预处理\n",
    "\n",
    "一篇文章可以看作字符或单词的序列，常见文本数据的预处理一般包括4个步骤：\n",
    "1. 读入文本\n",
    "2. 分词\n",
    "3. 建立字典，将单词映射成索引\n",
    "4. 将文本从词的序列转为索引的序列\n",
    "\n",
    "以下以英文小说[Time Machine](http://www.gutenberg.org/ebooks/35)为例，英文。"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 1,
   "metadata": {},
   "outputs": [],
   "source": [
    "import collections\n",
    "import re\n",
    "import nltk"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## 读入文本"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 2,
   "metadata": {},
   "outputs": [],
   "source": [
    "def read_time_machine():\n",
    "    with open(\"timemachine.txt\",\"r\",encoding=\"utf-8\") as f:\n",
    "        # 仅保留a-zA-Z的字符\n",
    "        lines = [re.sub('[^a-z]+',' ',line.strip().lower()) for line in f]\n",
    "    return lines"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 3,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "3702"
      ]
     },
     "execution_count": 3,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "lines = read_time_machine()\n",
    "len(lines)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## 分词\n",
    "字-**词**-句-**文章**\n",
    "\n",
    "**第一层列表并不是句子，因为输入并不是一个句子一行。**"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 4,
   "metadata": {},
   "outputs": [],
   "source": [
    "def tokenize(sentences, token=\"word\"):\n",
    "    if token == \"word\":\n",
    "        return [sentence.split(\" \") for sentence in sentences \n",
    "                                        if sentence.strip() != \"\" and \n",
    "                                           len(sentence.split(\" \"))>1]\n",
    "    elif token == \"char\":\n",
    "        return [list(sentence) for sentence in sentences]\n",
    "    else:\n",
    "        print(\"Error：unknown token type\"+token)        "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 5,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "['the', 'time', 'machine', 'by', 'h', 'erbert', 'g', 'eorge', 'wells', '']"
      ]
     },
     "execution_count": 5,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "tokens = tokenize(lines)\n",
    "tokens[0]"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## 建立字典"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 6,
   "metadata": {},
   "outputs": [],
   "source": [
    "def count_corpus(sentences):\n",
    "    tokens = [tk for st in sentences for tk in st]\n",
    "    return collections.Counter(tokens)  # 返回一个字典，记录每个词的出现次数"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 7,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "4932"
      ]
     },
     "execution_count": 7,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "len(count_corpus(tokens))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 8,
   "metadata": {},
   "outputs": [],
   "source": [
    "class Vocab():\n",
    "    def __init__(self, tokens, min_freq=0, use_special_tokens=False):\n",
    "        counter = count_corpus(tokens)\n",
    "        self.token_freqs = list(counter.items())\n",
    "        self.idx_to_token = []\n",
    "        if use_special_tokens:\n",
    "            # padding, begin of sentence, end of sentence, unknown\n",
    "            self.pad, self.bos, self.eos, self.unk = (0, 1, 2, 3)\n",
    "            self.idx_to_token += ['', '', '', '']\n",
    "        else:\n",
    "            self.unk = 0\n",
    "            self.idx_to_token += ['']\n",
    "        self.idx_to_token += [token for token, freq in self.token_freqs\n",
    "                        if freq >= min_freq and token not in self.idx_to_token]\n",
    "        self.token_to_idx = dict()\n",
    "        for idx, token in enumerate(self.idx_to_token):\n",
    "            self.token_to_idx[token] = idx\n",
    "    \n",
    "    def __len__(self):\n",
    "        return len(self.idx_to_token)\n",
    "    \n",
    "    def __getitem__(self, tokens):\n",
    "        if not isinstance(tokens, (list, tuple)):\n",
    "            return self.token_to_idx.get(tokens, self.unk)\n",
    "        return [self.__getitem__(token) for token in tokens]\n",
    "    \n",
    "    def to_tokens(self, indices):\n",
    "        if not isinstance(indices, (list, tuple)):\n",
    "            return self.idx_to_token[indices]\n",
    "        return [self.idx_to_token[index] for index in indices]"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 9,
   "metadata": {},
   "outputs": [],
   "source": [
    "vocab = Vocab(tokens)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 10,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "[('', 0), ('the', 1), ('time', 2), ('machine', 3), ('by', 4), ('h', 5), ('erbert', 6), ('g', 7), ('eorge', 8), ('wells', 9)]\n"
     ]
    }
   ],
   "source": [
    "print(list(vocab.token_to_idx.items())[0:10])"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 11,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "[1, 2]"
      ]
     },
     "execution_count": 11,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "vocab[[\"the\",\"time\"]]"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 12,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "['time', 'machine']"
      ]
     },
     "execution_count": 12,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "vocab.to_tokens([2,3])"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## 将词转为索引"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 31,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "line 0 : 10\n",
      "words: ['the', 'time', 'machine', 'by', 'h', 'erbert', 'g', 'eorge', 'wells', '']\n",
      "indices: [1, 2, 3, 4, 5, 6, 7, 8, 9, 0]\n",
      "line 1 : 12\n",
      "words: ['the', 'time', 'traveller', 'for', 'so', 'it', 'will', 'be', 'convenient', 'to', 'speak', 'of']\n",
      "indices: [1, 2, 10, 11, 12, 13, 14, 15, 16, 17, 18, 19]\n",
      "line 2 : 11\n",
      "words: ['him', 'was', 'expounding', 'a', 'recondite', 'matter', 'to', 'us', 'his', 'grey', 'eyes']\n",
      "indices: [20, 21, 22, 23, 24, 25, 17, 26, 27, 28, 29]\n",
      "line 3 : 11\n",
      "words: ['shone', 'and', 'twinkled', 'and', 'his', 'usually', 'pale', 'face', 'was', 'flushed', 'and']\n",
      "indices: [30, 31, 32, 31, 27, 33, 34, 35, 21, 36, 31]\n",
      "line 4 : 11\n",
      "words: ['animated', 'the', 'fire', 'burned', 'brightly', 'and', 'the', 'soft', 'radiance', 'of', 'the']\n",
      "indices: [37, 1, 38, 39, 40, 31, 1, 41, 42, 19, 1]\n",
      "line 5 : 10\n",
      "words: ['incandescent', 'lights', 'in', 'the', 'lilies', 'of', 'silver', 'caught', 'the', 'bubbles']\n",
      "indices: [43, 44, 45, 1, 46, 19, 47, 48, 1, 49]\n",
      "line 6 : 11\n",
      "words: ['that', 'flashed', 'and', 'passed', 'in', 'our', 'glasses', 'our', 'chairs', 'being', 'his']\n",
      "indices: [50, 51, 31, 52, 45, 53, 54, 53, 55, 56, 27]\n",
      "line 7 : 11\n",
      "words: ['patents', 'embraced', 'and', 'caressed', 'us', 'rather', 'than', 'submitted', 'to', 'be', 'sat']\n",
      "indices: [57, 58, 31, 59, 26, 60, 61, 62, 17, 15, 63]\n",
      "line 8 : 10\n",
      "words: ['upon', 'and', 'there', 'was', 'that', 'luxurious', 'after', 'dinner', 'atmosphere', 'when']\n",
      "indices: [64, 31, 65, 21, 50, 66, 67, 68, 69, 70]\n",
      "line 9 : 10\n",
      "words: ['thought', 'roams', 'gracefully', 'free', 'of', 'the', 'trammels', 'of', 'precision', 'and']\n",
      "indices: [71, 72, 73, 74, 19, 1, 75, 19, 76, 31]\n"
     ]
    }
   ],
   "source": [
    "for i in range(0, 10):\n",
    "    print(\"line {} :\".format(i),len(tokens[i]))\n",
    "    print('words:', tokens[i])\n",
    "    print('indices:', vocab[tokens[i]])"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## 用现有工具进行分词\n",
    "增加的功能\n",
    "- 标点符号的语义信息\n",
    "- doesn't等缩略词的处理"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 14,
   "metadata": {},
   "outputs": [],
   "source": [
    "text = \"Mr. Chen doesn't agree with my suggestion.\""
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### spacy"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 15,
   "metadata": {},
   "outputs": [],
   "source": [
    "import spacy\n",
    "# python -m spacy download en_core_web_sm"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 19,
   "metadata": {},
   "outputs": [],
   "source": [
    "nlp = spacy.load('en_core_web_sm')\n",
    "doc = nlp(text)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 17,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "['Mr.', 'Chen', 'does', \"n't\", 'agree', 'with', 'my', 'suggestion', '.']\n"
     ]
    }
   ],
   "source": [
    "print([token.text for token in doc])"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### nltk"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 28,
   "metadata": {},
   "outputs": [],
   "source": [
    "import nltk\n",
    "from nltk.tokenize import word_tokenize\n",
    "from nltk import data"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 29,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "['Mr.', 'Chen', 'does', \"n't\", 'agree', 'with', 'my', 'suggestion', '.']\n"
     ]
    }
   ],
   "source": [
    "print(word_tokenize(text))"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# 语言模型\n",
    "\n",
    "一段自然语言文本可以看作是一个离散时间序列，给定一个长度为T的词的序列，语言模型的目标是评估该序列是否合理，即计算该序列的概率。\n",
    "\n",
    "- 语料库：corpus\n",
    "\n",
    "语言模型的参数是**词的概率**以及**给定前n个词的情况下的条件概率**"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## n元语法（n-gram）\n",
    "\n",
    "n元语法通过马尔可夫假设简化模型，马尔可夫假设是指一个词的出现只与前面(n-1个相关，即n阶马尔科夫链。\n",
    "\n",
    "语言模型的n阶马尔可夫链，即n-gram。\n",
    "\n",
    "$$\n",
    "P(w_1, w_2, \\ldots, w_T) \\approx \\prod_{t=1}^T P(w_t \\mid w_{t-(n-1)}, \\ldots, w_{t-1}) .\n",
    "$$\n",
    "\n",
    "【常用的n-gram模型】\n",
    "- unigram：(2)\n",
    "- bigram：(3)\n",
    "- trigram\n",
    "\n",
    "$$\\begin{split}\\begin{aligned}\n",
    "P(w_1, w_2, w_3, w_4) &=  P(w_1) P(w_2) P(w_3) P(w_4) ,\\\\\n",
    "P(w_1, w_2, w_3, w_4) &=  P(w_1) P(w_2 \\mid w_1) P(w_3 \\mid w_2) P(w_4 \\mid w_3) ,\\\\\n",
    "P(w_1, w_2, w_3, w_4) &=  P(w_1) P(w_2 \\mid w_1) P(w_3 \\mid w_1, w_2) P(w_4 \\mid w_2, w_3) .\n",
    "\\end{aligned}\\end{split}$$\n",
    "\n",
    "【n-gram缺陷】\n",
    "1. 参数空间过大\n",
    "2. 数据过于稀疏(齐夫定律）"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## 周杰伦歌词数据集\n",
    "从[d2lzh的github](https://github.com/d2l-ai/d2l-zh/blob/master/data/jaychou_lyrics.txt.zip)上下载。"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 35,
   "metadata": {},
   "outputs": [],
   "source": [
    "import zipfile\n",
    "with zipfile.ZipFile('jaychou_lyrics.txt.zip') as zin:\n",
    "    with zin.open('jaychou_lyrics.txt') as f:\n",
    "        corpus_chars = f.read().decode('utf-8')"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 38,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "10000\n",
      "想要有直升机 想要和你飞到宇宙去 想要和你融化在一起 融化在宇宙里 我每天每天每\n"
     ]
    }
   ],
   "source": [
    "print(len(corpus_chars))\n",
    "print(corpus_chars[: 40])\n",
    "corpus_chars = corpus_chars.replace('\\n', ' ').replace('\\r', ' ')\n",
    "corpus_chars = corpus_chars[: 10000]"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### 建立字符索引"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 40,
   "metadata": {},
   "outputs": [],
   "source": [
    "idx_to_char = list(set(corpus_chars))\n",
    "char_to_idx = {char:i for i,char in enumerate(idx_to_char)}\n",
    "vocab_size = len(char_to_idx)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 42,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "1027"
      ]
     },
     "execution_count": 42,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "vocab_size"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 45,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "chars: 想要有直升机 想要和你飞到宇宙去 想要和\n",
      "indices: [833, 272, 249, 217, 146, 124, 763, 833, 272, 2, 901, 573, 897, 620, 493, 693, 763, 833, 272, 2]\n"
     ]
    }
   ],
   "source": [
    "corpus_indices = [char_to_idx[char] for char in corpus_chars]  # 将每个字符转化为索引，得到一个索引的序列\n",
    "sample = corpus_indices[: 20]\n",
    "print('chars:', ''.join([idx_to_char[idx] for idx in sample]))\n",
    "print('indices:', sample)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 47,
   "metadata": {},
   "outputs": [],
   "source": [
    "def load_data_jay_lyrics():\n",
    "    with zipfile.ZipFile('jaychou_lyrics.txt.zip') as zin:\n",
    "        with zin.open('jaychou_lyrics.txt') as f:\n",
    "            corpus_chars = f.read().decode('utf-8')\n",
    "    corpus_chars = corpus_chars.replace('\\n', ' ').replace('\\r', ' ')\n",
    "    corpus_chars = corpus_chars[0:10000]\n",
    "    idx_to_char = list(set(corpus_chars))\n",
    "    char_to_idx = dict([(char, i) for i, char in enumerate(idx_to_char)])\n",
    "    vocab_size = len(char_to_idx)\n",
    "    corpus_indices = [char_to_idx[char] for char in corpus_chars]\n",
    "    return corpus_indices, char_to_idx, idx_to_char, vocab_size"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## 时序数据的采样\n",
    "时间序列有大量重复的样本，因此需要采样。\n",
    "\n",
    "序列长度为T，时间步数为n，共有$T-n$个合法样本。\n",
    "\n",
    "- 随机采样\n",
    "- 相邻采样"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### 随机采样\n",
    "\n",
    "下面的代码每次从数据里随机采样一个小批量。其中批量大小`batch_size`是每个小批量的样本数，`num_steps`是每个样本所包含的时间步数。\n",
    "在随机采样中，每个样本是原始序列上任意截取的一段序列，相邻的两个随机小批量在原始序列上的位置不一定相毗邻。\n",
    "\n",
    "**硬性分成了多个片段，片段之间的联系破坏了。**"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 55,
   "metadata": {},
   "outputs": [],
   "source": [
    "import random\n",
    "import torch"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 56,
   "metadata": {},
   "outputs": [],
   "source": [
    "def data_iter_random(corpus_indices, batch_size, num_steps, device=None):\n",
    "    num_examples = (len(corpus_indices)-1)//num_steps\n",
    "    # example_indices记录每个分组的第一个下标\n",
    "    example_indices = [i*num_steps for i in range(num_examples)]\n",
    "    random.shuffle(example_indices)\n",
    "    \n",
    "    def _data(i):\n",
    "        return corpus_indices[i:i+num_steps]\n",
    "    if device is None:\n",
    "        device = torch.device(\"cuda\" if torch.cuda.is_available() else \"cpu\" )\n",
    "    for i in range(0,num_examples, batch_size):\n",
    "        batch_indices = example_indices[i:i+batch_size]\n",
    "        X = [_data(j) for j in batch_indices]\n",
    "        Y = [_data(j + 1) for j in batch_indices]\n",
    "        yield torch.tensor(X, device=device), torch.tensor(Y, device=device)\n",
    "        "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 57,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "X:  tensor([[12, 13, 14, 15, 16, 17],\n",
      "        [18, 19, 20, 21, 22, 23]], device='cuda:0') \n",
      "Y: tensor([[13, 14, 15, 16, 17, 18],\n",
      "        [19, 20, 21, 22, 23, 24]], device='cuda:0') \n",
      "\n",
      "X:  tensor([[ 6,  7,  8,  9, 10, 11],\n",
      "        [ 0,  1,  2,  3,  4,  5]], device='cuda:0') \n",
      "Y: tensor([[ 7,  8,  9, 10, 11, 12],\n",
      "        [ 1,  2,  3,  4,  5,  6]], device='cuda:0') \n",
      "\n"
     ]
    }
   ],
   "source": [
    "my_seq = list(range(30))\n",
    "for X, Y in data_iter_random(my_seq, batch_size=2, num_steps=6):\n",
    "    print('X: ', X, '\\nY:', Y, '\\n')"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### 相邻采样\n",
    "\n",
    "在相邻采样中，相邻的两个随机小批量在原始序列上的位置相毗邻。\n",
    "\n",
    "也把corpus序列分成了多个片段。"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 58,
   "metadata": {},
   "outputs": [],
   "source": [
    "def data_iter_consecutive(corpus_indices, batch_size, num_steps, device=None):\n",
    "    if device is None:\n",
    "        device = torch.device('cuda' if torch.cuda.is_available() else 'cpu')\n",
    "    corpus_len = len(corpus_indices) // batch_size * batch_size  # 保留下来的序列的长度\n",
    "    corpus_indices = corpus_indices[: corpus_len]  # 仅保留前corpus_len个字符\n",
    "    indices = torch.tensor(corpus_indices, device=device)\n",
    "    indices = indices.view(batch_size, -1)  # resize成(batch_size, )\n",
    "    batch_num = (indices.shape[1] - 1) // num_steps\n",
    "    for i in range(batch_num):\n",
    "        i = i * num_steps\n",
    "        X = indices[:, i: i + num_steps]\n",
    "        Y = indices[:, i + 1: i + num_steps + 1]\n",
    "        yield X, Y"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 61,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "X:  tensor([[ 0,  1,  2,  3,  4,  5],\n",
      "        [15, 16, 17, 18, 19, 20]], device='cuda:0') \n",
      "Y: tensor([[ 1,  2,  3,  4,  5,  6],\n",
      "        [16, 17, 18, 19, 20, 21]], device='cuda:0') \n",
      "\n",
      "X:  tensor([[ 6,  7,  8,  9, 10, 11],\n",
      "        [21, 22, 23, 24, 25, 26]], device='cuda:0') \n",
      "Y: tensor([[ 7,  8,  9, 10, 11, 12],\n",
      "        [22, 23, 24, 25, 26, 27]], device='cuda:0') \n",
      "\n"
     ]
    }
   ],
   "source": [
    "for X, Y in data_iter_consecutive(my_seq, batch_size=2, num_steps=6):\n",
    "    print('X: ', X, '\\nY:', Y, '\\n')"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# 循环神经网络基础"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.7.4"
  },
  "toc": {
   "base_numbering": 1,
   "nav_menu": {},
   "number_sections": true,
   "sideBar": true,
   "skip_h1_title": false,
   "title_cell": "Table of Contents",
   "title_sidebar": "Contents",
   "toc_cell": false,
   "toc_position": {},
   "toc_section_display": true,
   "toc_window_display": false
  }
 },
 "nbformat": 4,
 "nbformat_minor": 2
}

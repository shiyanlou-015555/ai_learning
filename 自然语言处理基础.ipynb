{
 "cells": [
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# 导包"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 1,
   "metadata": {},
   "outputs": [],
   "source": [
    "import torch\n",
    "import torchvision\n",
    "import torch.nn\n",
    "import torch.nn.init\n",
    "import torch.utils.data as data\n",
    "# print(torch.__version__)\n",
    "# print(torchvision.__version__)\n",
    "# print(torch.cuda.is_available())\n",
    "import numpy as np\n",
    "import random\n",
    "\n",
    "import collections\n",
    "import re\n",
    "import nltk"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 2,
   "metadata": {},
   "outputs": [],
   "source": [
    "import nltk\n",
    "import spacy"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 3,
   "metadata": {},
   "outputs": [],
   "source": [
    "import data"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# 文本预处理简介\n",
    "\n",
    "一篇文章可以看作字符或单词的序列，常见文本数据的预处理一般包括4个步骤：\n",
    "1. 读入文本\n",
    "2. 分词\n",
    "3. 建立字典，将单词映射成索引\n",
    "4. 将文本从词的序列转为索引的序列"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## STEP1：读入文本"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 4,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "# sentences 3221\n"
     ]
    }
   ],
   "source": [
    "def read_time_machine():\n",
    "    \"\"\"\n",
    "    获取小说《timemachine》\n",
    "    :return: timechine各行文本组成的列表。\n",
    "    \"\"\"\n",
    "    with open('Datasets/timemachine.txt', 'r') as f:\n",
    "        lines = [re.sub('[^a-z]+', ' ', line.strip().lower()) for line in f]\n",
    "    return lines\n",
    "\n",
    "lines = read_time_machine()\n",
    "print('# sentences %d' % len(lines))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 5,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "['the time machine by h g wells ', '']"
      ]
     },
     "execution_count": 5,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "lines[0:2]"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## STEP2：分词"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 6,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "[['the', 'time', 'machine', 'by', 'h', 'g', 'wells', ''], ['']]"
      ]
     },
     "execution_count": 6,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "def tokenize(sentences, token='word'):\n",
    "    \"\"\"\n",
    "    对文章进行分词，其中文章由句子组成，其实句子分成单词或者字符。\n",
    "    :param sentences: 文章句子组成的列表。\n",
    "    :param token: 分词模式，默认按单词分，比标点符号也算单词。\n",
    "    :return: 列表的列表，分词列表组成句子，句子组成文章。\n",
    "    \"\"\"\n",
    "    if token == 'word':\n",
    "        return [sentence.split(' ') for sentence in sentences]\n",
    "    elif token == 'char':\n",
    "        return [list(sentence) for sentence in sentences]\n",
    "    else:\n",
    "        print('ERROR: unkown token type '+token)\n",
    "\n",
    "sentences = tokenize(lines)\n",
    "sentences[0:2]"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## STEP3：建立字典"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 7,
   "metadata": {},
   "outputs": [],
   "source": [
    "def count_corpus(sentences):\n",
    "    \"\"\"\n",
    "    建立分词集。\n",
    "    :param sentences: 分好词的句子组成的列表。\n",
    "    :return: 分词集合。\n",
    "    \"\"\"\n",
    "    tokens = [tk for st in sentences for tk in st]\n",
    "    return collections.Counter(tokens)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 8,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "4580"
      ]
     },
     "execution_count": 8,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "tokens_Collect = count_corpus(sentences)\n",
    "len(tokens_Collect)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 9,
   "metadata": {},
   "outputs": [],
   "source": [
    "class Vocab(object):\n",
    "    \"\"\"\n",
    "    建立语料库的符号字典。\n",
    "    * self.idx_to_token：列表，编号 -> 符号。\n",
    "    * self.token_to_idx：字典，符号 -> 编号。\n",
    "    * len(vocab)\n",
    "    * vocab[token]、vocab[token_list]\n",
    "    * vocab.to_tokens(idx)、vocab.to_tokens(idx_list)\n",
    "    \"\"\"\n",
    "    def __init__(self, sentences, min_freq=0, use_special_tokens=False):\n",
    "        \"\"\"\n",
    "        语料库字典初始化。\n",
    "        :param tokens: 句子集合。\n",
    "        :param min_freq: 分词超过该数量才建立索引。\n",
    "        :param use_special_tokens: 使用特殊含义符号，如空格、句子开始、句子结束，否则只有未知符号。\n",
    "        \"\"\"\n",
    "        counter = count_corpus(sentences)  \n",
    "        self.token_freqs = list(counter.items())\n",
    "        self.idx_to_token = []\n",
    "        if use_special_tokens:\n",
    "            # padding, begin of sentence, end of sentence, unknown\n",
    "            self.pad, self.bos, self.eos, self.unk = (0, 1, 2, 3)\n",
    "            self.idx_to_token += ['', '', '', '']\n",
    "        else:\n",
    "            self.unk = 0\n",
    "            self.idx_to_token += ['']\n",
    "        self.idx_to_token += [token for token, freq in self.token_freqs if freq >= min_freq and token not in self.idx_to_token]\n",
    "        self.token_to_idx = dict()\n",
    "        for idx, token in enumerate(self.idx_to_token):\n",
    "            self.token_to_idx[token] = idx\n",
    "\n",
    "    def __len__(self):\n",
    "        return len(self.idx_to_token)\n",
    "\n",
    "    def __getitem__(self, tokens):\n",
    "        if not isinstance(tokens, (list, tuple)):\n",
    "            return self.token_to_idx.get(tokens, self.unk)\n",
    "        return [self.__getitem__(token) for token in tokens]\n",
    "\n",
    "    def to_tokens(self, indices):\n",
    "        if not isinstance(indices, (list, tuple)):\n",
    "            return self.idx_to_token[indices]\n",
    "        return [self.idx_to_token[index] for index in indices]"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 10,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "[('', 0), ('the', 1), ('time', 2), ('machine', 3), ('by', 4), ('h', 5), ('g', 6), ('wells', 7), ('i', 8), ('traveller', 9)]\n"
     ]
    }
   ],
   "source": [
    "vocab = Vocab(sentences)\n",
    "print(list(vocab.token_to_idx.items())[0:10])"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## STEP4：将词转换为索引"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 11,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "words: ['the', 'time', 'traveller', 'for', 'so', 'it', 'will', 'be', 'convenient', 'to', 'speak', 'of', 'him', '']\n",
      "indices: [1, 2, 9, 10, 11, 12, 13, 14, 15, 16, 17, 18, 19, 0]\n",
      "words: ['was', 'expounding', 'a', 'recondite', 'matter', 'to', 'us', 'his', 'grey', 'eyes', 'shone', 'and']\n",
      "indices: [20, 21, 22, 23, 24, 16, 25, 26, 27, 28, 29, 30]\n"
     ]
    }
   ],
   "source": [
    "for i in range(8,10):\n",
    "    print(\"words:\",sentences[i])\n",
    "    print(\"indices:\",vocab[sentences[i]])"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## 自然语言工具包\n",
    "- `spacy`\n",
    "- `nltk`"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 12,
   "metadata": {},
   "outputs": [],
   "source": [
    "text = \"Mr. Chen doesn't agree with my suggestion.\""
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### spacy"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 13,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "['Mr.', 'Chen', 'does', \"n't\", 'agree', 'with', 'my', 'suggestion', '.']\n"
     ]
    }
   ],
   "source": [
    "# python -m spacy download en_core_web_sm\n",
    "nlp = spacy.load('en_core_web_sm')\n",
    "doc = nlp(text)\n",
    "print([token.text for token in doc])"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### nltk"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 14,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "['Mr.', 'Chen', 'does', \"n't\", 'agree', 'with', 'my', 'suggestion', '.']"
      ]
     },
     "execution_count": 14,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "# nltk.download('punkt')\n",
    "nltk.tokenize.word_tokenize(text)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### jieba：中文分词"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# 语言模型\n",
    "\n",
    "一段自然语言文本可以看作是一个离散时间序列，给定一个长度为T的词的序列，语言模型的目标是评估该序列是否合理，即计算该序列的概率。\n",
    "\n",
    "- 语料库：corpus\n",
    "\n",
    "语言模型的参数是**词的概率**以及**给定前n个词的情况下的条件概率**。"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## n元语法（n-gram）\n",
    "\n",
    "n元语法通过马尔可夫假设简化模型，马尔可夫假设是指一个词的出现只与前面(n-1个相关，即n阶马尔科夫链。\n",
    "\n",
    "语言模型的n阶马尔可夫链，即n-gram。\n",
    "\n",
    "$$\n",
    "P(w_1, w_2, \\ldots, w_T) \\approx \\prod_{t=1}^T P(w_t \\mid w_{t-(n-1)}, \\ldots, w_{t-1}) .\n",
    "$$\n",
    "\n",
    "### 常用的n-gram模型\n",
    "- unigram\n",
    "$$ P(w_1, w_2, w_3, w_4) =  P(w_1) P(w_2) P(w_3) P(w_4) $$\n",
    "- bigram\n",
    "$$ P(w_1, w_2, w_3, w_4) =  P(w_1) P(w_2 \\mid w_1) P(w_3 \\mid w_2) P(w_4 \\mid w_3) $$\n",
    "- trigram\n",
    "$$ P(w_1, w_2, w_3, w_4)  =  P(w_1) P(w_2 \\mid w_1) P(w_3 \\mid w_1, w_2) P(w_4 \\mid w_2, w_3) $$\n",
    "\n",
    "### n-gram的缺陷\n",
    "1. 参数空间过大\n",
    "2. 数据过于稀疏(齐夫定律）"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# 时序数据采样\n",
    "经过预处理后的字符索引序列是连续的，要经过采样才能输入RNN网络。\n",
    "\n",
    "序列长度为T，时间步数为n，共有$T-n$个合法样本。\n",
    "\n",
    "- 随机采样\n",
    "- 相邻采样\n",
    "\n",
    "**采样后的形状是`（batch_size，num_steps）`**"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## 随机采样\n",
    "随机采样批量大小`batch_size`是每个小批量的样本数，`num_steps`是每个样本所包含的时间步数。\n",
    "\n",
    "**X似乎必须在间隔为num_steps的分割点上。**\n",
    "\n",
    "在随机采样中，每个样本是原始序列上任意截取的一段序列，相邻的两个随机小批量在原始序列上的位置不一定相毗邻，因此，我们无法用一个小批量最终时间步的隐藏状态来初始化下一个小批量的隐藏状态。\n",
    "\n",
    "在训练模型时，每次随机采样前都需要重新初始化隐藏状态。"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 15,
   "metadata": {},
   "outputs": [],
   "source": [
    "def data_iter_random(corpus_indices, batch_size, num_steps, device=None):\n",
    "    \"\"\"\n",
    "    时序数据随机采样，样本不重叠。\n",
    "\n",
    "    :param corpus_indices: 时序数据列表。\n",
    "    :param batch_size: 批量大小，\n",
    "    :param num_steps: 采样的时间步数。\n",
    "    :param device: CUDA / CPU。\n",
    "    :return: 迭代器（tensorX, tensorY）。\n",
    "    \"\"\"\n",
    "    num_examples = (len(corpus_indices) - 1) // num_steps  \n",
    "    example_indices = [i * num_steps for i in range(num_examples)] \n",
    "    random.shuffle(example_indices)\n",
    "\n",
    "    def _data(i):\n",
    "        return corpus_indices[i: i + num_steps]\n",
    "\n",
    "    if device is None:\n",
    "        device = torch.device('cuda' if torch.cuda.is_available() else 'cpu')\n",
    "    for i in range(0, num_examples // batch_size * batch_size, batch_size):\n",
    "        batch_indices = example_indices[i: i + batch_size] \n",
    "        X = [_data(j) for j in batch_indices]\n",
    "        Y = [_data(j + 1) for j in batch_indices]\n",
    "        yield torch.tensor(X, device=device), torch.tensor(Y, device=device)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 16,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "X:  tensor([[24, 25, 26, 27, 28, 29],\n",
      "        [18, 19, 20, 21, 22, 23]], device='cuda:0') \n",
      "Y: tensor([[25, 26, 27, 28, 29, 30],\n",
      "        [19, 20, 21, 22, 23, 24]], device='cuda:0') \n",
      "\n",
      "X:  tensor([[12, 13, 14, 15, 16, 17],\n",
      "        [ 6,  7,  8,  9, 10, 11]], device='cuda:0') \n",
      "Y: tensor([[13, 14, 15, 16, 17, 18],\n",
      "        [ 7,  8,  9, 10, 11, 12]], device='cuda:0') \n",
      "\n"
     ]
    }
   ],
   "source": [
    "my_seq = list(range(31))\n",
    "# X起始点 0, 6, 12, 18, 24\n",
    "for X, Y in data_iter_random(my_seq, batch_size=2, num_steps=6):\n",
    "    print('X: ', X, '\\nY:', Y, '\\n')"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## 相邻采样\n",
    "\n",
    "先把时序数据按`batch_size`折叠。\n",
    "\n",
    "在相邻采样中，一批内的样本间隔相等，相邻的两个随机小批量在原始序列上的位置相毗邻，因此，可以用一个小批量最终时间步的隐藏状态来初始化下一个小批量的隐藏状态，从而使下一个小批量的输出也取决于当前小批量的输入，并如此循环下去，在训练模型时，我们只需在每一个迭代周期开始时初始化隐藏状态。\n",
    "\n",
    "当多个相邻小批量通过传递隐藏状态串联起来时，模型参数的梯度计算将依赖所有串联起来的小批量序列。同一迭代周期中，随着迭代次数的增加，梯度的计算开销会越来越大。 为了使模型参数的梯度计算只依赖一次迭代读取的小批量序列，我们可以在每次读取小批量前将隐藏状态从计算图中分离出来。"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 17,
   "metadata": {},
   "outputs": [],
   "source": [
    "def data_iter_consecutive(corpus_indices, batch_size, num_steps, device=None):\n",
    "    \"\"\"\n",
    "    时序数据随机采样，样本不重叠，一批内数据间隔相等，相邻的两个随机小批量在原始序列上的位置相毗邻。\n",
    "\n",
    "    :param corpus_indices: 时序数据列表。\n",
    "    :param batch_size: 批量大小，\n",
    "    :param num_steps: 采样的时间步数。\n",
    "    :param device: CUDA / CPU。\n",
    "    :return: 迭代器（tensorX, tensorY）。\n",
    "    \"\"\"\n",
    "    if device is None:\n",
    "        device = torch.device('cuda' if torch.cuda.is_available() else 'cpu')\n",
    "    corpus_len = len(corpus_indices) // batch_size * batch_size\n",
    "    corpus_indices = corpus_indices[: corpus_len]\n",
    "    indices = torch.tensor(corpus_indices, device=device)\n",
    "    indices = indices.view(batch_size, -1)\n",
    "    batch_num = (indices.shape[1] - 1) // num_steps\n",
    "    for i in range(batch_num):\n",
    "        i = i * num_steps\n",
    "        X = indices[:, i: i + num_steps]\n",
    "        Y = indices[:, i + 1: i + num_steps + 1]\n",
    "        yield X, Y"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 18,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "X:  tensor([[ 0,  1,  2,  3,  4,  5],\n",
      "        [15, 16, 17, 18, 19, 20]], device='cuda:0') \n",
      "Y: tensor([[ 1,  2,  3,  4,  5,  6],\n",
      "        [16, 17, 18, 19, 20, 21]], device='cuda:0') \n",
      "\n",
      "X:  tensor([[ 6,  7,  8,  9, 10, 11],\n",
      "        [21, 22, 23, 24, 25, 26]], device='cuda:0') \n",
      "Y: tensor([[ 7,  8,  9, 10, 11, 12],\n",
      "        [22, 23, 24, 25, 26, 27]], device='cuda:0') \n",
      "\n"
     ]
    }
   ],
   "source": [
    "# 0  6\n",
    "# 15 21\n",
    "for X, Y in data_iter_consecutive(my_seq, batch_size=2, num_steps=6):\n",
    "    print('X: ', X, '\\nY:', Y, '\\n')"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# 示例"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.7.4"
  },
  "toc": {
   "base_numbering": 1,
   "nav_menu": {},
   "number_sections": true,
   "sideBar": true,
   "skip_h1_title": false,
   "title_cell": "Table of Contents",
   "title_sidebar": "Contents",
   "toc_cell": false,
   "toc_position": {},
   "toc_section_display": true,
   "toc_window_display": true
  }
 },
 "nbformat": 4,
 "nbformat_minor": 2
}

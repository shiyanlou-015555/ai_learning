{
 "cells": [
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# 导包"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 73,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "1.4.0\n",
      "0.5.0\n",
      "True\n"
     ]
    }
   ],
   "source": [
    "import torch\n",
    "import torchvision\n",
    "import torch.nn\n",
    "import torch.nn.init\n",
    "print(torch.__version__)\n",
    "print(torchvision.__version__)\n",
    "print(torch.cuda.is_available())\n",
    "import numpy as np\n",
    "import random\n",
    "import time\n",
    "import matplotlib.pyplot as plt\n",
    "import collections\n",
    "import pickle\n",
    "import math"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 2,
   "metadata": {},
   "outputs": [],
   "source": [
    "import nlp\n",
    "import rnn\n",
    "import basic\n",
    "import data"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 3,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "<module 'nlp' from 'C:\\\\Users\\\\bambo\\\\PycharmProjects\\\\ai_learning\\\\nlp.py'>"
      ]
     },
     "execution_count": 3,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "import imp\n",
    "imp.reload(nlp)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# 机器翻译及相关技术\n",
    "机器翻译的特别之处在于输入和输出都是不定长的。"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## 数据准备\n",
    "### 读入文本\n",
    "1. 读取文本\n",
    "2. 替换法文空格为普通空格\n",
    "\n",
    "字符在计算机里是以编码的形式存在，我们通常所用的空格是 `\\x20` ，是在标准ASCII可见字符 `0x20~0x7e` 范围内。\n",
    "而 `\\xa0` 属于 `latin1 （ISO/IEC_8859-1）`中的扩展字符集字符，代表不间断空白符`nbsp(non-breaking space)`，超出`gbk`编码范围，是需要去除的特殊字符。"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 4,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Go.\tVa !\tCC-BY 2.0 (France) Attribution: tatoeba.org #2877272 (CM) & #1158250 (Wittydev)\n",
      "Hi.\tSalut !\tCC-BY 2.0 (France) Attribution: tatoeba.org #538123 (CM) & #509819 (Aiji)\n",
      "Hi.\tSalut.\tCC-BY 2.0 (France) Attribution: tatoeba.org #538123 (CM) & #4320462 (gillux)\n",
      "Run!\tCours !\tCC-BY 2.0 (France) Attribution: tatoeba.org #906328 (papabear) & #906331 (sacredceltic)\n",
      "Run!\tCourez !\tCC-BY 2.0 (France) Attribution: tatoeba.org #906328 (papabear) & #906332 (sacredceltic)\n",
      "Who?\tQui ?\tCC-BY 2.0 (France) Attribution: tatoeba.org #2083030 (CK) & #4366796 (gillux)\n",
      "Wow!\tÇa alors !\tCC-BY 2.0 (France) Attribution: tatoeba.org #52027 (Zifre) & #374631 (zmoo)\n",
      "Fire!\tAu feu !\tCC-BY 2.0 (France) Attribution: tatoeba.org #1829639 (Spamster) & #4627939 (sacredceltic)\n",
      "Help!\tÀ l'aide !\tCC-BY 2.0 (France) Attribution: tatoeba.org #435084 (lukaszpp) & #128430 (sysko)\n",
      "Jump.\tSaute.\tCC-BY 2.0 (France) Attribution: tatoeba.org #631038 (Shishir) & #2416938 (Phoenix)\n",
      "Stop!\tÇa suffit !\tCC-BY 2.0 (France) Attribution: tato\n"
     ]
    }
   ],
   "source": [
    "with open(\"DataSets/fra.txt\",encoding=\"utf-8\") as f:\n",
    "    raw_text = f.read()\n",
    "print(raw_text[0:1000])"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "字典的结构是\n",
    "- 英语token\n",
    "- 法语token\n",
    "- 其他"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 5,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "25666314"
      ]
     },
     "execution_count": 5,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "len(raw_text)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 6,
   "metadata": {},
   "outputs": [],
   "source": [
    "def preprocess_raw(text):\n",
    "    text = text.replace(\"\\u202f\",\" \").replace(\"\\xa0\",\" \")\n",
    "    out = \"\"\n",
    "    for i, char in enumerate(text.lower()):\n",
    "        if char in (\",\",\"!\",\".\") and i>0 and text[i-1]!=\" \":\n",
    "            out += \" \"\n",
    "        out += char\n",
    "    return out"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 7,
   "metadata": {},
   "outputs": [],
   "source": [
    "# text = preprocess_raw(raw_text)\n",
    "# print(text[0:1000])\n",
    "# import pickle\n",
    "# with open(\"Datasets/fr_text.pkl\",\"wb\") as f:\n",
    "#     pickle.dump(text, f)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 8,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "go .\tva !\tcc-by 2 .0 (france) attribution: tatoeba .org #2877272 (cm) & #1158250 (wittydev)\n",
      "hi .\tsalut !\tcc-by 2 .0 (france) attribution: tatoeba .org #538123 (cm) & #509819 (aiji)\n",
      "hi .\tsalut .\tcc-by 2 .0 (france) attribution: tatoeba .org #538123 (cm) & #4320462 (gillux)\n",
      "run !\tcours !\tcc-by 2 .0 (france) attribution: tatoeba .org #906328 (papabear) & #906331 (sacredceltic)\n",
      "run !\tcourez !\tcc-by 2 .0 (france) attribution: tatoeba .org #906328 (papabear) & #906332 (sacredceltic)\n",
      "who?\tqui ?\tcc-by 2 .0 (france) attribution: tatoeba .org #2083030 (ck) & #4366796 (gillux)\n",
      "wow !\tça alors !\tcc-by 2 .0 (france) attribution: tatoeba .org #52027 (zifre) & #374631 (zmoo)\n",
      "fire !\tau feu !\tcc-by 2 .0 (france) attribution: tatoeba .org #1829639 (spamster) & #4627939 (sacredceltic)\n",
      "help !\tà l'aide !\tcc-by 2 .0 (france) attribution: tatoeba .org #435084 (lukaszpp) & #128430 (sysko)\n",
      "jump .\tsaute .\tcc-by 2 .0 (france) attribution: tatoeba .org #631038 (shishir) & #2416938 (phoenix)\n",
      "stop !\tça suffit !\tcc-b\n"
     ]
    }
   ],
   "source": [
    "with open(\"Datasets/fr_text.pkl\",\"rb\") as f:\n",
    "    text = pickle.load(f)\n",
    "print(text[0:1000])"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### 分词\n",
    "文章字符串 -- 单词组成的（句子）的列表"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 9,
   "metadata": {},
   "outputs": [],
   "source": [
    "num_examples = 50000\n",
    "source, target = [],[]\n",
    "for i, line in enumerate(text.split(\"\\n\")):\n",
    "    if i > num_examples:\n",
    "        break\n",
    "    parts = line.split(\"\\t\")\n",
    "    if len(parts) >= 2:\n",
    "        source.append(parts[0].split(\" \"))\n",
    "        target.append(parts[1].split(\" \"))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 10,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "([['go', '.'], ['hi', '.'], ['hi', '.']],\n",
       " [['va', '!'], ['salut', '!'], ['salut', '.']])"
      ]
     },
     "execution_count": 10,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "source[:3], target[:3]"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 11,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "<matplotlib.legend.Legend at 0x25bd54e7c88>"
      ]
     },
     "execution_count": 11,
     "metadata": {},
     "output_type": "execute_result"
    },
    {
     "data": {
      "image/png": "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\n",
      "text/plain": [
       "<Figure size 432x288 with 1 Axes>"
      ]
     },
     "metadata": {
      "needs_background": "light"
     },
     "output_type": "display_data"
    }
   ],
   "source": [
    "plt.rcParams[\"figure.figsize\"] = (6,4)\n",
    "plt.hist([[len(l) for l in source], [len(l) for l in target]], label=[\"source\", \"target\"])\n",
    "plt.legend(loc=\"upper right\")"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### 建立字典\n",
    "`单词` → `id`的列表。"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 39,
   "metadata": {},
   "outputs": [],
   "source": [
    "class Vocab_dict(object):\n",
    "    \"\"\"\n",
    "    建立字典的语料库，单词级别。\n",
    "    \"\"\"\n",
    "    def __init__(self, tokens, min_freq=0, use_special_tokens=False):\n",
    "        counter = collections.Counter(tokens)\n",
    "        token_freqs = sorted(counter.items(), key=lambda x:x[0])\n",
    "        token_freqs.sort(key=(lambda x:x[1]), reverse=True)\n",
    "        if use_special_tokens:\n",
    "            self.pad, self.bos, self.eos, self.unk = (0,1,2,3)\n",
    "            tokens = [\"<pad>\",\"<bos>\",\"<eos>\",\"<unk>\"]\n",
    "        else:\n",
    "            self.unk = 0\n",
    "            tokens = [\"<unk>\"]\n",
    "        tokens += [token for token, freq in token_freqs if freq >= min_freq]\n",
    "        self.idx_to_token = []\n",
    "        self.token_to_idx = dict()\n",
    "        for token in tokens:\n",
    "            self.idx_to_token.append(token)\n",
    "            self.token_to_idx[token] = len(self.idx_to_token) - 1\n",
    "\n",
    "    def __len__(self):\n",
    "        return len(self.idx_to_token)\n",
    "\n",
    "    def __getitem__(self, items):\n",
    "        if not isinstance(items, (list, tuple)):\n",
    "            return self.token_to_idx.get(items, self.unk)\n",
    "        else:\n",
    "            return [self.__getitem__(item) for item in items]\n",
    "        \n",
    "    def to_tokens(self, indices):\n",
    "        if not isinstance(indices, (list, tuple)):\n",
    "            return self.idx_to_token[indices]\n",
    "        return [self.idx_to_token[index] for index in indices]"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 13,
   "metadata": {},
   "outputs": [],
   "source": [
    "def build_vocab(tokens):\n",
    "    \"\"\"\n",
    "    从字典建立语料库。\n",
    "    :param tokens:\n",
    "    :return:\n",
    "    \"\"\"\n",
    "    tokens = [token for line in tokens for token in line]\n",
    "    return nlp.Vocab_dict(tokens, min_freq=3, use_special_tokens=True)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 14,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "3789"
      ]
     },
     "execution_count": 14,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "src_vocab = build_vocab(source)\n",
    "len(src_vocab)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### 字符索引映射&长度补全"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 15,
   "metadata": {},
   "outputs": [],
   "source": [
    "def pad(line, max_len, padding_token):\n",
    "    \"\"\"\n",
    "    句子截断&补全。\n",
    "    :param line: 分好词的句子。\n",
    "    :param max_len: 最大长度。\n",
    "    :param padding_token: 空白字符。\n",
    "    :return: \n",
    "    \"\"\"\n",
    "    if len(line) > max_len:\n",
    "        return line[:max_len]\n",
    "    return line + [padding_token] * (max_len - len(line))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 16,
   "metadata": {},
   "outputs": [],
   "source": [
    "def build_array(lines, vocab, max_len, is_source):\n",
    "    \"\"\"\n",
    "    建立字典的每一行从token转换为idx。\n",
    "    :param lines: 字典。\n",
    "    :param vocab: 字典语料库。\n",
    "    :param max_len: 句子最大长度。\n",
    "    :param is_source: 是否标注起始、结束。\n",
    "    :return: array, valid_len\n",
    "    \"\"\"\n",
    "    lines = [vocab[line] for line in lines]\n",
    "    if not is_source:\n",
    "        lines = [[vocab.bos] + line + [vocab.eos] for line in lines]\n",
    "    array = torch.tensor([pad(line, max_len, vocab.pad) for line in lines])\n",
    "    valid_len = (array != vocab.pad).sum(dim=1)\n",
    "    return array, valid_len"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 17,
   "metadata": {},
   "outputs": [],
   "source": [
    "def load_data_nmt(source, target, batch_size, max_len): \n",
    "    \"\"\"\n",
    "    生成机器翻译字典的批量数据。\n",
    "    :param batch_size: 批量大小。\n",
    "    :param max_len: 句子长度上线。\n",
    "    :return: 迭代器\n",
    "    \"\"\"\n",
    "    src_vocab, tgt_vocab = nlp.build_vocab(source), nlp.build_vocab(target)\n",
    "    src_array, src_valid_len = nlp.build_array(source, src_vocab, max_len, True)\n",
    "    tgt_array, tgt_valid_len = nlp.build_array(target, tgt_vocab, max_len, False)\n",
    "    train_data = torch.utils.data.TensorDataset(src_array, src_valid_len, tgt_array, tgt_valid_len)\n",
    "    train_iter = torch.utils.data.DataLoader(train_data, batch_size, shuffle=True)\n",
    "    return src_vocab, tgt_vocab, train_iter"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 18,
   "metadata": {},
   "outputs": [],
   "source": [
    "src_vocab, tgt_vocab, train_iter = load_data_nmt(source, target, batch_size=2, max_len=8)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 19,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "X = tensor([[ 195,   11,   10,   18,    4,    0,    0,    0],\n",
      "        [  14,  213,   36,    7, 3233,    4,    0,    0]], dtype=torch.int32) \n",
      "Valid lengths for X = tensor([5, 6]) \n",
      "Y = tensor([[   1,    3,   13,    2,    0,    0,    0,    0],\n",
      "        [   1,   11, 1102,   20,   27,    3,    4,    2]], dtype=torch.int32) \n",
      "Valid lengths for Y = tensor([4, 8])\n"
     ]
    }
   ],
   "source": [
    "for X, X_valid_len, Y, Y_valid_len, in train_iter:\n",
    "    print('X =', X.type(torch.int32), '\\nValid lengths for X =', X_valid_len,\n",
    "        '\\nY =', Y.type(torch.int32), '\\nValid lengths for Y =', Y_valid_len)\n",
    "    break"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## encoder-decoder\n",
    "![encoder-decode结构](https://cdn.kesci.com/upload/image/q5jcat3c8m.png?imageView2/0/w/640/h/640)\n",
    "\n",
    "- encoder：输入到隐藏状态\n",
    "- decoder：隐藏状态到输出"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 20,
   "metadata": {},
   "outputs": [],
   "source": [
    "class Encoder(torch.nn.Module):\n",
    "    def __init__(self, **kwargs):\n",
    "        super(Encoder, self).__init__(**kwargs)\n",
    "\n",
    "    def forward(self, X, *args):\n",
    "        raise NotImplementedError"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 21,
   "metadata": {},
   "outputs": [],
   "source": [
    "class Decoder(torch.nn.Module):\n",
    "    def __init__(self, **kwargs):\n",
    "        super(Decoder, self).__init__(**kwargs)\n",
    "\n",
    "    def init_state(self, enc_outputs, *args):\n",
    "        raise NotImplementedError\n",
    "\n",
    "    def forward(self, X, state):\n",
    "        raise NotImplementedError"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 22,
   "metadata": {},
   "outputs": [],
   "source": [
    "class EncoderDecoder(torch.nn.Module):\n",
    "    def __init__(self, encoder, decoder, **kwargs):\n",
    "        super(EncoderDecoder, self).__init__(**kwargs)\n",
    "        self.encoder = encoder\n",
    "        self.decoder = decoder\n",
    "\n",
    "    def forward(self, enc_X, dec_X, *args):\n",
    "        enc_outputs = self.encoder(enc_X, *args)\n",
    "        dec_state = self.decoder.init_state(enc_outputs, *args)\n",
    "        return self.decoder(dec_X, dec_state)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## seq2seq\n",
    "- 预测时decoder的输暑促和是下一步的输入。\n",
    "- encoder的隐藏层状态传递给decoder。\n",
    "\n",
    "### 模型训练  \n",
    "![Image Name](https://cdn.kesci.com/upload/image/q5jc7a53pt.png?imageView2/0/w/640/h/640)\n",
    "\n",
    "### 模型预测\n",
    "![Image Name](https://cdn.kesci.com/upload/image/q5jcecxcba.png?imageView2/0/w/640/h/640)\n",
    "\n",
    "\n",
    "\n",
    "### 具体结构：\n",
    "输入先做embedding。\n",
    "\n",
    "![Image Name](https://cdn.kesci.com/upload/image/q5jccjhkii.png?imageView2/0/w/500/h/500)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "#### Encoder\n",
    "本编码器以LSTM（`torch.nn.LSTM`）为基础：\n",
    "- 输入X： (batch_size, seq_len, embed_size)；\n",
    "- 输出Y： (seq_len, batch_size, num_hiddens)；\n",
    "- LSTM的state： (num_layers, batch_size, num_hiddens)。\n",
    "    - 隐藏状态\n",
    "    - 记忆细胞，"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 23,
   "metadata": {},
   "outputs": [],
   "source": [
    "class Seq2SeqEncoder(Encoder):\n",
    "    \"\"\"\n",
    "    Encoder的具体实现。\n",
    "    X shape: (batch_size, seq_len, embed_size)；\n",
    "    Y shape: (seq_len, batch_size, num_hiddens)；\n",
    "    LSTM的state包含最后一步的隐藏状态、记忆细胞，shape是 (num_layers, batch_size, num_hiddens)。\n",
    "    \"\"\"\n",
    "    def __init__(self, vocab_size, embed_size, num_hiddens, num_layers, dropout=0, **kwargs):\n",
    "        super(Seq2SeqEncoder, self).__init__(**kwargs)\n",
    "        self.num_hiddens = num_hiddens\n",
    "        self.num_layers = num_layers\n",
    "        self.embedding = torch.nn.Embedding(vocab_size, embed_size)\n",
    "        self.rnn = torch.nn.LSTM(embed_size, num_hiddens, num_layers, dropout=dropout)\n",
    "\n",
    "    def begin_state(self, batch_size, device):\n",
    "        return [torch.zeros(size=(self.num_layers, batch_size, self.num_hiddens), device=device),\n",
    "                torch.zeros(size=(self.num_layers, batch_size, self.num_hiddens), device=device)]\n",
    "\n",
    "    def forward(self, X, *args):\n",
    "        X = self.embedding(X)  # X shape: (batch_size, seq_len, embed_size)\n",
    "        X = X.transpose(0, 1)  # RNN needs first axes to be time\n",
    "        # state = self.begin_state(X.shape[1], device=X.device)\n",
    "        out, state = self.rnn(X)\n",
    "        return out, state"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 24,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "(torch.Size([7, 4, 16]), 2, torch.Size([2, 4, 16]), torch.Size([2, 4, 16]))"
      ]
     },
     "execution_count": 24,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "encoder = Seq2SeqEncoder(vocab_size=10, embed_size=8,num_hiddens=16, num_layers=2)\n",
    "X = torch.zeros((4, 7),dtype=torch.long)\n",
    "output, state = encoder(X)\n",
    "output.shape, len(state), state[0].shape, state[1].shape"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "#### Decoder"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 25,
   "metadata": {},
   "outputs": [],
   "source": [
    "class Seq2SeqDecoder(Decoder):\n",
    "    def __init__(self, vocab_size, embed_size, num_hiddens, num_layers, dropout=0, **kwargs):\n",
    "        super(Seq2SeqDecoder, self).__init__(**kwargs)\n",
    "        self.embedding = torch.nn.Embedding(vocab_size, embed_size)\n",
    "        self.rnn = torch.nn.LSTM(embed_size,num_hiddens, num_layers, dropout=dropout)\n",
    "        self.dense = torch.nn.Linear(num_hiddens,vocab_size)\n",
    "\n",
    "    def init_state(self, enc_outputs, *args):\n",
    "        return enc_outputs[1]\n",
    "\n",
    "    def forward(self, X, state):\n",
    "        X = self.embedding(X).transpose(0, 1)\n",
    "        out, state = self.rnn(X, state)\n",
    "        # Make the batch to be the first dimension to simplify loss computation.\n",
    "        out = self.dense(out).transpose(0, 1)\n",
    "        return out, state"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 26,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "(torch.Size([4, 7, 10]), 2, torch.Size([2, 4, 16]), torch.Size([2, 4, 16]))"
      ]
     },
     "execution_count": 26,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "decoder = Seq2SeqDecoder(vocab_size=10, embed_size=8,num_hiddens=16, num_layers=2)\n",
    "state = decoder.init_state(encoder(X))\n",
    "out, state = decoder(X, state)\n",
    "out.shape, len(state), state[0].shape, state[1].shape"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## 损失函数\n",
    "仅在有效长度求损失"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### 有效长度掩码"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 27,
   "metadata": {},
   "outputs": [],
   "source": [
    "def SequenceMask(X, X_len,value=0):\n",
    "    \"\"\"\n",
    "    对于不定长序列求损失函数，仅在有效长度进行。\n",
    "    :param X: (batch_size, maxlen)每一行是一个序列数据。\n",
    "    :param X_len: (batch_size,)有效长度。\n",
    "    :param value: mask。\n",
    "    :return: \n",
    "    \"\"\"\n",
    "    maxlen = X.size(1)\n",
    "    mask = torch.arange(maxlen)[None, :].to(X_len.device) < X_len[:, None]   \n",
    "    X[~mask]=value\n",
    "    return X"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 28,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "tensor([[1, 0, 0],\n",
       "        [4, 5, 0]])"
      ]
     },
     "execution_count": 28,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "X = torch.tensor([[1,2,3], [4,5,6]])\n",
    "SequenceMask(X,torch.tensor([1,2]))"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### 带掩码的损失函数"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 29,
   "metadata": {},
   "outputs": [],
   "source": [
    "class MaskedSoftmaxCELoss(torch.nn.CrossEntropyLoss):\n",
    "    \"\"\"\n",
    "    带掩码的交叉熵损失函数类。\n",
    "    \n",
    "    X shape: (batch_size, seq_len, vocab_size)\n",
    "    y shape: (batch_size, seq_len)\n",
    "    valid_length shape: (batch_size, )\n",
    "    \"\"\"\n",
    "    def forward(self, pred, label, valid_length):\n",
    "        # the sample weights shape should be (batch_size, seq_len)\n",
    "        weights = torch.ones_like(label)\n",
    "        weights = SequenceMask(weights, valid_length).float()\n",
    "        self.reduction='none'\n",
    "        # (batch_size, vocab_size, seq_len) x (batch_size, seq_len) -> (batch_size, seq_len)\n",
    "        output=super(MaskedSoftmaxCELoss, self).forward(pred.transpose(1,2), label)\n",
    "        return (output*weights).mean(dim=1)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 30,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "tensor([2.3026, 1.7269, 0.0000])"
      ]
     },
     "execution_count": 30,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "loss = MaskedSoftmaxCELoss()\n",
    "loss(torch.ones((3, 4, 10)), torch.ones((3,4),dtype=torch.long), torch.tensor([4,3,0]))"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## 训练"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 31,
   "metadata": {},
   "outputs": [],
   "source": [
    "def grad_clipping_nn(model, theta, device):\n",
    "    grad_clipping(model.parameters(), theta, device)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 32,
   "metadata": {},
   "outputs": [],
   "source": [
    "def train_ch7(model, data_iter, lr, num_epochs, device):\n",
    "    model.to(device)\n",
    "    optimizer = torch.optim.Adam(model.parameters(), lr=lr)\n",
    "    loss = MaskedSoftmaxCELoss()\n",
    "    tic = time.time()\n",
    "    for epoch in range(1, num_epochs + 1):\n",
    "        l_sum, num_tokens_sum = 0.0, 0.0\n",
    "        for batch in data_iter:\n",
    "            optimizer.zero_grad()\n",
    "            X, X_vlen, Y, Y_vlen = [x.to(device) for x in batch]\n",
    "            Y_input, Y_label, Y_vlen = Y[:, :-1], Y[:, 1:], Y_vlen - 1\n",
    "            Y_hat, _ = model(X, Y_input, X_vlen, Y_vlen)\n",
    "            \n",
    "            l = loss(Y_hat, Y_label, Y_vlen).sum()\n",
    "            l.backward()\n",
    "            with torch.no_grad():\n",
    "                rnn.grad_clipping(model.parameters(), 5, device)\n",
    "            num_tokens = Y_vlen.sum().item()\n",
    "            optimizer.step()\n",
    "            \n",
    "            l_sum += l.sum().item()\n",
    "            num_tokens_sum += num_tokens\n",
    "        if epoch % 50 == 0:\n",
    "            print(\"epoch {0:4d},loss {1:.3f}, time {2:.1f} sec\".format(\n",
    "                epoch, (l_sum / num_tokens_sum), time.time() - tic))\n",
    "            tic = time.time()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 33,
   "metadata": {},
   "outputs": [],
   "source": [
    "embed_size, num_hiddens, num_layers, dropout = 32, 32, 2, 0.0\n",
    "batch_size, num_examples, max_len = 64, 1e3, 10"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 34,
   "metadata": {},
   "outputs": [],
   "source": [
    "lr, num_epochs, ctx = 0.005, 300, basic.try_gpu()\n",
    "src_vocab, tgt_vocab, train_iter = data.load_data_nmt(source, target, batch_size, max_len)\n",
    "encoder = Seq2SeqEncoder(len(src_vocab), embed_size, num_hiddens, num_layers, dropout)\n",
    "decoder = Seq2SeqDecoder(len(tgt_vocab), embed_size, num_hiddens, num_layers, dropout)\n",
    "model = EncoderDecoder(encoder, decoder)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 35,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "epoch   50,loss 0.145, time 717.4 sec\n",
      "epoch  100,loss 0.127, time 723.9 sec\n",
      "epoch  150,loss 0.120, time 728.2 sec\n",
      "epoch  200,loss 0.116, time 714.1 sec\n",
      "epoch  250,loss 0.113, time 715.9 sec\n",
      "epoch  300,loss 0.114, time 716.4 sec\n"
     ]
    }
   ],
   "source": [
    "train_ch7(model, train_iter, lr, num_epochs, ctx)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## 测试"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 57,
   "metadata": {},
   "outputs": [],
   "source": [
    "def translate_ch7(model, src_sentence, src_vocab, tgt_vocab, max_len, device):\n",
    "    \"\"\"\n",
    "    机器翻译的enc-dec预测。\n",
    "    :param model: encoder-decode模型。\n",
    "    :param src_sentence: 待翻译语句。\n",
    "    :param src_vocab: 源语言词典。\n",
    "    :param tgt_vocab: 目标语言词典。\n",
    "    :param max_len: 最大有效句子长度。\n",
    "    :param device: CPU/GPU。\n",
    "    :return: 翻译好的语句。\n",
    "    \"\"\"\n",
    "    src_tokens = src_vocab[src_sentence.lower().split(' ')]\n",
    "    src_len = len(src_tokens)\n",
    "    if src_len < max_len:\n",
    "        src_tokens += [src_vocab.pad] * (max_len - src_len)\n",
    "    enc_X = torch.tensor(src_tokens, device=device)\n",
    "    enc_valid_length = torch.tensor([src_len], device=device)\n",
    "    enc_outputs = model.encoder(enc_X.unsqueeze(dim=0), enc_valid_length)\n",
    "    dec_state = model.decoder.init_state(enc_outputs, enc_valid_length)\n",
    "    dec_X = torch.tensor([tgt_vocab.bos], device=device).unsqueeze(dim=0)\n",
    "    predict_tokens = []\n",
    "    for _ in range(max_len):\n",
    "        Y, dec_state = model.decoder(dec_X, dec_state)\n",
    "        # The token with highest score is used as the next time step input.\n",
    "        dec_X = Y.argmax(dim=2)\n",
    "        py = dec_X.squeeze(dim=0).int().item()\n",
    "        if py == tgt_vocab.eos:\n",
    "            break\n",
    "        predict_tokens.append(py)\n",
    "    return [src_vocab.idx_to_token[i] for i in predict_tokens]"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 63,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Go . <unk> i'm\n",
      "Wow ! come be young what temporary .\n",
      "I'm OK . i car me? .\n",
      "I won ! don't often .\n"
     ]
    }
   ],
   "source": [
    "for sentence in ['Go .', 'Wow !', \"I'm OK .\", 'I won !']:\n",
    "    print(sentence + \" \" + \" \".join(translate_ch7(model, sentence, src_vocab, tgt_vocab, max_len, ctx)))"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## 集束搜索\n",
    "贪心搜搜，挑选较好的几个继续预测。"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# 注意力机制及Seq2Seq模型\n",
    "\n",
    "## 注意力机制\n",
    "Attention 是一种通用的带权池化方法，输入由两部分构成：询问（query）和键值对（key-value pairs）。$𝐤_𝑖∈ℝ^{𝑑_𝑘}, 𝐯_𝑖∈ℝ^{𝑑_𝑣}$. Query  $𝐪∈ℝ^{𝑑_𝑞}$ , attention layer得到输出与value的维度一致 $𝐨∈ℝ^{𝑑_𝑣}$. 对于一个query来说，attention layer 会与每一个key计算注意力分数并进行权重的归一化，输出的向量$o$则是value的加权求和，而每个key计算的权重与value一一对应。\n",
    "\n",
    "为了计算输出，我们首先假设有一个函数$\\alpha$ 用于计算query和key的相似性，然后可以计算所有的 attention scores $a_1, \\ldots, a_n$ by\n",
    "\n",
    "\n",
    "$$\n",
    "a_i = \\alpha(\\mathbf q, \\mathbf k_i).\n",
    "$$\n",
    "\n",
    "\n",
    "我们使用 softmax函数 获得注意力权重：\n",
    "\n",
    "\n",
    "$$\n",
    "b_1, \\ldots, b_n = \\textrm{softmax}(a_1, \\ldots, a_n).\n",
    "$$\n",
    "\n",
    "\n",
    "最终的输出就是value的加权求和：\n",
    "\n",
    "\n",
    "$$\n",
    "\\mathbf o = \\sum_{i=1}^n b_i \\mathbf v_i.\n",
    "$$\n",
    "\n",
    "\n",
    "![Image Name](https://cdn.kesci.com/upload/image/q5km4ooyu2.PNG?imageView2/0/w/960/h/960)\n",
    "\n",
    "注意力机制的不一样在于$a_i = \\alpha(\\mathbf q, \\mathbf k_i)$不同，常见的注意层有\n",
    "- `Dot-product Attention`\n",
    "- `Multilayer Perceptron Attention`\n",
    "\n",
    "`torch.bmm()`：batch matric multiply"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 64,
   "metadata": {},
   "outputs": [],
   "source": [
    "def SequenceMask(X, X_len,value=-1e6):\n",
    "    maxlen = X.size(1)\n",
    "    #print(X.size(),torch.arange((maxlen),dtype=torch.float)[None, :],'\\n',X_len[:, None] )\n",
    "    mask = torch.arange((maxlen),dtype=torch.float)[None, :] >= X_len[:, None]   \n",
    "    #print(mask)\n",
    "    X[mask]=value\n",
    "    return X"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 75,
   "metadata": {},
   "outputs": [],
   "source": [
    "def masked_softmax(X, valid_length):\n",
    "    # X: 3-D tensor, valid_length: 1-D or 2-D tensor\n",
    "    softmax = torch.nn.Softmax(dim=-1)\n",
    "    if valid_length is None:\n",
    "        return softmax(X)\n",
    "    else:\n",
    "        shape = X.shape\n",
    "        if valid_length.dim() == 1:\n",
    "            try:\n",
    "                valid_length = torch.FloatTensor(valid_length.numpy().repeat(shape[1], axis=0))#[2,2,3,3]\n",
    "            except:\n",
    "                valid_length = torch.FloatTensor(valid_length.cpu().numpy().repeat(shape[1], axis=0))#[2,2,3,3]\n",
    "        else:\n",
    "            valid_length = valid_length.reshape((-1,))\n",
    "        # fill masked elements with a large negative, whose exp is 0\n",
    "        X = SequenceMask(X.reshape((-1, shape[-1])), valid_length)\n",
    " \n",
    "        return softmax(X).reshape(shape)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## 点积注意力\n",
    "The dot product 假设query和keys有相同的维度, 即 $\\forall i, 𝐪,𝐤_𝑖 ∈ ℝ_𝑑 $. 通过计算query和key转置的乘积来计算attention score,通常还会除去 $\\sqrt{d}$ 减少计算出来的score对维度𝑑的依赖性，如下\n",
    "\n",
    "\n",
    "$$\n",
    "𝛼(𝐪,𝐤)=⟨𝐪,𝐤⟩/ \\sqrt{d} \n",
    "$$\n",
    "\n",
    "假设 $ 𝐐∈ℝ^{𝑚×𝑑}$ 有 $m$ 个query，$𝐊∈ℝ^{𝑛×𝑑}$ 有 $n$ 个keys. 我们可以通过矩阵运算的方式计算所有 $mn$ 个score：\n",
    "\n",
    "\n",
    "$$\n",
    "𝛼(𝐐,𝐊)=𝐐𝐊^𝑇/\\sqrt{d}\n",
    "$$\n",
    " \n",
    "现在让我们实现这个层，它支持一批查询和键值对。此外，它支持作为正则化随机删除一些注意力权重.\n",
    "\n",
    "`torch.bmm`"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 76,
   "metadata": {},
   "outputs": [],
   "source": [
    "class DotProductAttention(torch.nn.Module): \n",
    "    def __init__(self, dropout, **kwargs):\n",
    "        super(DotProductAttention, self).__init__(**kwargs)\n",
    "        self.dropout = torch.nn.Dropout(dropout)\n",
    "\n",
    "    # query: (batch_size, #queries, d)\n",
    "    # key: (batch_size, #kv_pairs, d)\n",
    "    # value: (batch_size, #kv_pairs, dim_v)\n",
    "    # valid_length: either (batch_size, ) or (batch_size, xx)\n",
    "    def forward(self, query, key, value, valid_length=None):\n",
    "        d = query.shape[-1]\n",
    "        # set transpose_b=True to swap the last two dimensions of key\n",
    "        \n",
    "        scores = torch.bmm(query, key.transpose(1,2)) / math.sqrt(d)\n",
    "        attention_weights = self.dropout(masked_softmax(scores, valid_length))\n",
    "        print(\"attention_weight\\n\",attention_weights)\n",
    "        return torch.bmm(attention_weights, value)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 77,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "attention_weight\n",
      " tensor([[[0.5000, 0.5000, 0.0000, 0.0000, 0.0000, 0.0000, 0.0000, 0.0000,\n",
      "          0.0000, 0.0000]],\n",
      "\n",
      "        [[0.1667, 0.1667, 0.1667, 0.1667, 0.1667, 0.1667, 0.0000, 0.0000,\n",
      "          0.0000, 0.0000]]])\n"
     ]
    },
    {
     "data": {
      "text/plain": [
       "tensor([[[ 2.0000,  3.0000,  4.0000,  5.0000]],\n",
       "\n",
       "        [[10.0000, 11.0000, 12.0000, 13.0000]]])"
      ]
     },
     "execution_count": 77,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "atten = DotProductAttention(dropout=0)\n",
    "\n",
    "keys = torch.ones((2,10,2),dtype=torch.float)\n",
    "values = torch.arange((40), dtype=torch.float).view(1,10,4).repeat(2,1,1)\n",
    "atten(torch.ones((2,1,2),dtype=torch.float), keys, values, torch.FloatTensor([2, 6]))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 78,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "(torch.Size([2, 10, 2]), torch.Size([2, 10, 4]))"
      ]
     },
     "execution_count": 78,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "keys.shape, values.shape"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## 多层感知机注意力层\n",
    "在多层感知器中，我们首先将 query and keys 投影到  $ℝ^ℎ$ .为了更具体，我们将可以学习的参数做如下映射 \n",
    "$𝐖_𝑘∈ℝ^{ℎ×𝑑_𝑘}$ ,  $𝐖_𝑞∈ℝ^{ℎ×𝑑_𝑞}$ , and  $𝐯∈ℝ^h$ . 将score函数定义\n",
    "$$\n",
    "𝛼(𝐤,𝐪)=𝐯^𝑇tanh(𝐖_𝑘𝐤+𝐖_𝑞𝐪)\n",
    "$$\n",
    ". \n",
    "然后将key 和 value 在特征的维度上合并（concatenate），然后送至 a single hidden layer perceptron 这层中 hidden layer 为  ℎ  and 输出的size为 1 .隐层激活函数为tanh，无偏置。\n",
    "\n",
    "尽管MLPAttention包含一个额外的MLP模型，但如果给定相同的输入和相同的键，我们将获得与DotProductAttention相同的输出。\n",
    "\n",
    "**在Dot-product Attention中，key与query维度需要一致，在MLP Attention中则不需要。**"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 79,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Save to the d2l package.\n",
    "class MLPAttention(torch.nn.Module):  \n",
    "    def __init__(self, units,ipt_dim,dropout, **kwargs):\n",
    "        super(MLPAttention, self).__init__(**kwargs)\n",
    "        # Use flatten=True to keep query's and key's 3-D shapes.\n",
    "        self.W_k = torch.nn.Linear(ipt_dim, units, bias=False)\n",
    "        self.W_q = torch.nn.Linear(ipt_dim, units, bias=False)\n",
    "        self.v = torch.nn.Linear(units, 1, bias=False)\n",
    "        self.dropout = torch.nn.Dropout(dropout)\n",
    "\n",
    "    def forward(self, query, key, value, valid_length):\n",
    "        query, key = self.W_k(query), self.W_q(key)\n",
    "        #print(\"size\",query.size(),key.size())\n",
    "        # expand query to (batch_size, #querys, 1, units), and key to\n",
    "        # (batch_size, 1, #kv_pairs, units). Then plus them with broadcast.\n",
    "        features = query.unsqueeze(2) + key.unsqueeze(1)\n",
    "        #print(\"features:\",features.size())  #--------------开启\n",
    "        scores = self.v(features).squeeze(-1) \n",
    "        attention_weights = self.dropout(masked_softmax(scores, valid_length))\n",
    "        return torch.bmm(attention_weights, value)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 80,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "tensor([[[ 2.0000,  3.0000,  4.0000,  5.0000]],\n",
       "\n",
       "        [[10.0000, 11.0000, 12.0000, 13.0000]]], grad_fn=<BmmBackward>)"
      ]
     },
     "execution_count": 80,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "atten = MLPAttention(ipt_dim=2,units = 8, dropout=0)\n",
    "atten(torch.ones((2,1,2), dtype = torch.float), keys, values, torch.FloatTensor([2, 6]))"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## seq2seq with attention\n",
    "\n",
    "\n",
    "\n",
    "![带注意力的seq2seq](https://cdn.kesci.com/upload/image/q5km8dihlr.PNG?imageView2/0/w/800/h/800)\n",
    "\n",
    "\n",
    "decoder输出作为注意力层的query，encoder输出作为注意力层的输入。"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# Transformer\n",
    "\n",
    "- CNNs 易于并行化，却不适合捕捉变长序列内的依赖关系。\n",
    "- RNNs 适合捕捉长距离变长序列的依赖，但是却难以实现并行化处理序列。\n",
    "\n",
    "Transformer同样基于编码器-解码器架构，其区别主要在于以下三点：\n",
    "1. Transformer blocks：将seq2seq模型重的循环网络替换为了Transformer Blocks，该模块包含一个多头注意力层（Multi-head Attention Layers）以及两个position-wise feed-forward networks（FFN）。对于解码器来说，另一个多头注意力层被用于接受编码器的隐藏状态。\n",
    "2. Add and norm：多头注意力层和前馈网络的输出被送到两个“add and norm”层进行处理，该层包含残差结构以及层归一化。\n",
    "3. Position encoding：由于自注意力层并没有区分元素的顺序，所以一个位置编码层被用于向序列元素里添加位置信息。\n",
    "\n",
    "![Fig. 10.3.1 The Transformer architecture.](https://cdn.kesci.com/upload/image/q5kpbj2cj5.png?imageView2/0/w/960/h/960)\n",
    "\n"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Transfomer Block"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Add and norm"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Position encoding"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.7.4"
  },
  "toc": {
   "base_numbering": 1,
   "nav_menu": {},
   "number_sections": true,
   "sideBar": true,
   "skip_h1_title": false,
   "title_cell": "Table of Contents",
   "title_sidebar": "Contents",
   "toc_cell": false,
   "toc_position": {
    "height": "calc(100% - 180px)",
    "left": "10px",
    "top": "150px",
    "width": "279.273px"
   },
   "toc_section_display": true,
   "toc_window_display": true
  }
 },
 "nbformat": 4,
 "nbformat_minor": 2
}
